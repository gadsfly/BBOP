{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== V1 指定会话对比较（不匹配 ROI 名称） ===\n",
      "\n",
      "===== 会话对 比较（不匹配 ROI 名）=====\n",
      "Single 会话: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21\n",
      "Social-close 会话: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_18/20240919v1l5r1mini_p20240717PMC_social_test_11_30\n",
      "\n",
      "--- 95th-% ΔF/F 分布（Single vs Social-close）---\n",
      "Single:       n=31, mean=2.968, std=0.813, median=2.849\n",
      "Social-close: n=42, mean=2.688, std=1.737, median=2.015\n",
      "Mann–Whitney U (ΔF/F): U=873.000, p-value=0.01344\n",
      "\n",
      "--- firing frequency (Hz) 分布（Single vs Social）---\n",
      "Single freq:       n=31, mean=1.833, std=0.290, median=1.896\n",
      "Social freq:       n=42, mean=1.689, std=0.456, median=1.686\n",
      "Mann–Whitney U (freq): U=822.000, p-value=0.05706\n",
      "\n",
      "===== 会话对 比较（不匹配 ROI 名）=====\n",
      "Single 会话: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54\n",
      "Social-close 会话: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_31/20240919v1l5r2mini_p20240717PMC_social_14_04\n",
      "\n",
      "--- 95th-% ΔF/F 分布（Single vs Social-close）---\n",
      "Single:       n=15, mean=3.917, std=1.588, median=3.459\n",
      "Social-close: n=17, mean=2.225, std=1.130, median=1.974\n",
      "Mann–Whitney U (ΔF/F): U=221.000, p-value=0.00044\n",
      "\n",
      "--- firing frequency (Hz) 分布（Single vs Social）---\n",
      "Single freq:       n=15, mean=1.944, std=0.519, median=2.028\n",
      "Social freq:       n=17, mean=1.416, std=0.324, median=1.392\n",
      "Mann–Whitney U (freq): U=203.000, p-value=0.00462\n",
      "\n",
      "所有统计计算完毕。\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Standalone script to compute:\n",
    " 1. 对 PMC 与 V1 整体的 95th-% ΔF/F（far, close, single）做描述性统计与非配对检验\n",
    " 2. 针对两对 V1 会话 (single vs social-close)，不匹配 ROI 名称，直接把各自所有 ROI 的 95th-% ΔF/F 作为两组独立样本，\n",
    "    分别做 Mann–Whitney U 检验（不配对）。\n",
    " 3. 对上述两对会话，分别计算每个 ROI 的近似 firing frequency（Hz），并用 Mann–Whitney U 检验比较频率分布。\n",
    "\n",
    "Usage:\n",
    "    python3 compute_v1_pairwise_no_roi_matching.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon, mannwhitneyu\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. 加载排除字典 & HDF5 读取函数\n",
    "# ----------------------------------------------------------------------------\n",
    "exclude_json_path = '/home/lq53/mir_repos/BBOP/random_tests/25feb_more_corr_explo/neuro_exclude.json'\n",
    "with open(exclude_json_path, 'r') as f:\n",
    "    exclude_mapping = json.load(f)\n",
    "\n",
    "def get_excluded_neurons_for_session(session_path, exclude_dict):\n",
    "    \"\"\"\n",
    "    如果 session_path 在排除字典里，就返回需要排除的神经元索引列表，否则返回空列表。\n",
    "    \"\"\"\n",
    "    if not isinstance(exclude_dict, dict):\n",
    "        return []\n",
    "    if session_path in exclude_dict:\n",
    "        return exclude_dict[session_path]\n",
    "    for key, val in exclude_dict.items():\n",
    "        if key in session_path:\n",
    "            return val\n",
    "    return []\n",
    "\n",
    "def load_session_data(rec_path):\n",
    "    \"\"\"\n",
    "    在 rec_path/MIR_Aligned 下寻找匹配的 .h5 文件，将其读入为 DataFrame 并添加 session_path 字段。\n",
    "    \"\"\"\n",
    "    h5_dir = os.path.join(rec_path, 'MIR_Aligned')\n",
    "    pattern = '*aligned_predictions_with_ca_and_dF_F*.h5'\n",
    "    h5_files = glob.glob(os.path.join(h5_dir, pattern))\n",
    "    if not h5_files:\n",
    "        raise FileNotFoundError(f\"No .h5 found in {h5_dir}\")\n",
    "    df = pd.read_hdf(h5_files[0], key='df')\n",
    "    if df.index.name == 'timestamp_ms_mini':\n",
    "        df = df.reset_index()\n",
    "    df['session_path'] = rec_path\n",
    "    return df\n",
    "\n",
    "def build_neuron_matrix_raw(df, exclude_dict):\n",
    "    \"\"\"\n",
    "    - 如果 df 的索引名是 'timestamp_ms_mini'，先 reset_index()\n",
    "    - 找到所有以 'dF_F_roi' 开头的列，排除 exclude_dict 中指定的 ROI 索引\n",
    "    - 计算每个 ROI 的方差，将底部 5% 方差的 ROI 丢弃\n",
    "    - 返回：kept_names (list[str]) 和 raw_matrix (numpy.ndarray, shape=(n_kept, n_frames))\n",
    "    \"\"\"\n",
    "    if df.index.name == 'timestamp_ms_mini':\n",
    "        df = df.reset_index()\n",
    "    sess = df['session_path'].iloc[0]\n",
    "    excluded = get_excluded_neurons_for_session(sess, exclude_dict)\n",
    "    all_names = [c for c in df.columns if c.startswith('dF_F_roi')]\n",
    "    keep_names = []\n",
    "    for c in all_names:\n",
    "        try:\n",
    "            idx = int(c.split('_')[-1][3:])\n",
    "        except:\n",
    "            continue\n",
    "        if idx not in excluded:\n",
    "            keep_names.append(c)\n",
    "    if not keep_names:\n",
    "        return [], np.zeros((0, len(df)))\n",
    "    activity = df[keep_names].values.T  # shape: (n_all, n_frames)\n",
    "    variances = np.var(activity, axis=1)\n",
    "    cutoff = np.percentile(variances, 5) if variances.size else 0\n",
    "    keep_mask = variances > cutoff\n",
    "    kept_names = [keep_names[i] for i in range(len(keep_names)) if keep_mask[i]]\n",
    "    filtered = activity[keep_mask, :]\n",
    "    return kept_names, filtered\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2a. 获取 social session 中每个 ROI 在 far/close 片段的 95th 百分位\n",
    "# ----------------------------------------------------------------------------\n",
    "def get_social_far_close_stats(rec_path, exclude_dict, threshold=250.0, pct=95):\n",
    "    \"\"\"\n",
    "    对一个 social session：\n",
    "      1. load DataFrame & build raw neuron matrix (n_kept, n_frames)\n",
    "      2. 从 CSV 读取每帧的距离，并和 HDF5 的 camera_frame_sixcam 对齐\n",
    "      3. 根据 threshold 分类成 far_mask (distance > threshold) 和 close_mask (distance <= threshold)\n",
    "      4. 对每个 ROI（行），分别计算 raw dF/F 在 far 和 close 片段中的第 pct 百分位数\n",
    "    返回：\n",
    "      roi_names: list[str]\n",
    "      stat_far:   numpy array of shape (n_kept,)\n",
    "      stat_close: numpy array of shape (n_kept,)\n",
    "    \"\"\"\n",
    "    df = load_session_data(rec_path)\n",
    "    roi_names, mat_raw = build_neuron_matrix_raw(df, exclude_dict)\n",
    "    if mat_raw.size == 0:\n",
    "        return [], np.array([]), np.array([])\n",
    "\n",
    "    com_csv_path = os.path.join(rec_path, 'MIR_Aligned', 'com_distances_filtered.csv')\n",
    "    if not os.path.exists(com_csv_path):\n",
    "        raise FileNotFoundError(f\"Missing {com_csv_path}\")\n",
    "\n",
    "    df_com = pd.read_csv(com_csv_path)\n",
    "    if df_com.shape[1] < 2:\n",
    "        raise KeyError(\"com_distances_filtered.csv 必须包含至少两列：帧索引和距离\")\n",
    "\n",
    "    frame_col = df_com.columns[0]\n",
    "    dist_col  = df_com.columns[1]\n",
    "    df_com[frame_col] = df_com[frame_col].astype(int)\n",
    "    df_com[dist_col]  = df_com[dist_col].astype(float)\n",
    "\n",
    "    if 'camera_frame_sixcam' not in df.columns:\n",
    "        raise KeyError(\"HDF5 必须包含 'camera_frame_sixcam' 列以便合并\")\n",
    "\n",
    "    merged = pd.merge(\n",
    "        df[['camera_frame_sixcam']].reset_index(drop=True),\n",
    "        df_com[[frame_col, dist_col]].rename(columns={frame_col: 'camera_frame_sixcam'}),\n",
    "        on='camera_frame_sixcam',\n",
    "        how='left'\n",
    "    )\n",
    "    distances = merged[dist_col].fillna(threshold + 1.0).values\n",
    "\n",
    "    close_mask = distances <= threshold\n",
    "    far_mask   = distances > threshold\n",
    "\n",
    "    if not np.any(close_mask):\n",
    "        stat_close = np.zeros(len(roi_names)) + np.nan\n",
    "    else:\n",
    "        stat_close = np.nanpercentile(mat_raw[:, close_mask], pct, axis=1)\n",
    "    if not np.any(far_mask):\n",
    "        stat_far = np.zeros(len(roi_names)) + np.nan\n",
    "    else:\n",
    "        stat_far = np.nanpercentile(mat_raw[:, far_mask], pct, axis=1)\n",
    "\n",
    "    return roi_names, stat_far, stat_close\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2b. 获取 single session 中每个 ROI 在全时段的 95th 百分位\n",
    "# ----------------------------------------------------------------------------\n",
    "def get_single_stats(rec_path, exclude_dict, pct=95):\n",
    "    \"\"\"\n",
    "    对一个 single session：\n",
    "      1. load DataFrame & build raw neuron matrix (n_kept, n_frames)\n",
    "      2. 对每个 ROI（行），计算 raw dF/F 在全时段中的第 pct 百分位数\n",
    "    返回：\n",
    "      roi_names: list[str]\n",
    "      stat_all:   numpy array of shape (n_kept,)\n",
    "    \"\"\"\n",
    "    df = load_session_data(rec_path)\n",
    "    roi_names, mat_raw = build_neuron_matrix_raw(df, exclude_dict)\n",
    "    if mat_raw.size == 0:\n",
    "        return [], np.array([])\n",
    "    stat_all = np.nanpercentile(mat_raw, pct, axis=1)\n",
    "    return roi_names, stat_all\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2c. 计算每个 ROI 的近似“firing frequency”（Hz）\n",
    "# ----------------------------------------------------------------------------\n",
    "def get_firing_frequency(rec_path, exclude_dict, threshold_factor=2.0, frame_rate=30.0):\n",
    "    \"\"\"\n",
    "    对一个会话：\n",
    "      1. load DataFrame & build raw neuron matrix (n_kept, n_frames)\n",
    "      2. 对每个 ROI，计算阈值 = median + threshold_factor * std\n",
    "      3. 统计超出阈值的帧数，除以总时长（假设 frame_rate 帧/秒），得到频率（Hz）\n",
    "    返回：\n",
    "      roi_names: list[str]\n",
    "      freqs:     numpy array of shape (n_kept,)\n",
    "    \"\"\"\n",
    "    df = load_session_data(rec_path)\n",
    "    roi_names, mat_raw = build_neuron_matrix_raw(df, exclude_dict)\n",
    "    if mat_raw.size == 0:\n",
    "        return [], np.array([])\n",
    "    medians = np.nanmedian(mat_raw, axis=1)\n",
    "    stds    = np.nanstd(mat_raw, axis=1)\n",
    "    thresholds = medians + threshold_factor * stds\n",
    "    n_frames = mat_raw.shape[1]\n",
    "    duration_sec = n_frames / frame_rate\n",
    "    if duration_sec <= 0:\n",
    "        freqs = np.zeros(len(roi_names)) + np.nan\n",
    "    else:\n",
    "        firing_counts = (mat_raw > thresholds[:, None]).sum(axis=1)\n",
    "        freqs = firing_counts / duration_sec\n",
    "    return roi_names, freqs\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3a. 聚合所有 social session 的 far & close 数据（用于整体统计）\n",
    "# ----------------------------------------------------------------------------\n",
    "def aggregate_social_far_close(social_sessions, exclude_dict, threshold=250.0, pct=95):\n",
    "    \"\"\"\n",
    "    对一组 social_sessions，循环调用 get_social_far_close_stats，\n",
    "    把每个 ROI 的 far_pct 和 close_pct 都添加到 all_far 和 all_close 列表里。\n",
    "    返回：\n",
    "      np.array(all_far), np.array(all_close)\n",
    "    \"\"\"\n",
    "    all_far = []\n",
    "    all_close = []\n",
    "\n",
    "    for rec_path in social_sessions:\n",
    "        try:\n",
    "            _, stat_far, stat_close = get_social_far_close_stats(rec_path, exclude_dict, threshold, pct)\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipping {rec_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        valid_idx = ~np.isnan(stat_far) & ~np.isnan(stat_close)\n",
    "        all_far.extend(stat_far[valid_idx].tolist())\n",
    "        all_close.extend(stat_close[valid_idx].tolist())\n",
    "\n",
    "    return np.array(all_far), np.array(all_close)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3b. 聚合所有 single session 的全部 95th 数据（用于整体统计）\n",
    "# ----------------------------------------------------------------------------\n",
    "def aggregate_single_stats(single_sessions, exclude_dict, pct=95):\n",
    "    \"\"\"\n",
    "    对一组 single_sessions，循环调用 get_single_stats，\n",
    "    把每个 ROI 的 pct 值添加到 all_single 列表里。\n",
    "    返回：\n",
    "      np.array(all_single)\n",
    "    \"\"\"\n",
    "    all_single = []\n",
    "\n",
    "    for rec_path in single_sessions:\n",
    "        try:\n",
    "            _, stat_all = get_single_stats(rec_path, exclude_dict, pct)\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipping {rec_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        valid_idx = ~np.isnan(stat_all)\n",
    "        all_single.extend(stat_all[valid_idx].tolist())\n",
    "\n",
    "    return np.array(all_single)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. 针对指定的两对 V1 会话，分别比较 single vs social-close（不匹配 ROI 名称）\n",
    "# ----------------------------------------------------------------------------\n",
    "def compute_session_pair_unpaired(pairs, exclude_dict, pct=95):\n",
    "    \"\"\"\n",
    "    对每个 (single_path, social_path) 对：\n",
    "      1. 分别获取 single 会话的 95% ΔF/F (roi_single_vals)\n",
    "      2. 获取 social 会话的 close-95% ΔF/F (roi_social_close_vals)\n",
    "      3. 直接将这两组视为独立样本，做 Mann–Whitney U 检验\n",
    "      4. 同时分别计算两会话中每个 ROI 的 firing frequency，然后两组频率也用 Mann–Whitney U 检验\n",
    "      5. 打印各自分布的中位数、均值、标准差，以及检验结果\n",
    "    \"\"\"\n",
    "    for single_path, social_path in pairs:\n",
    "        # 4.1 single 会话 95th ΔF/F\n",
    "        roi_single, stat_single = get_single_stats(single_path, exclude_dict, pct)\n",
    "        # 4.2 social 会话的 close-95th ΔF/F\n",
    "        roi_social, stat_far, stat_close = get_social_far_close_stats(social_path, exclude_dict, threshold=250.0, pct=pct)\n",
    "\n",
    "        # 如果两组数据都为空，则跳过\n",
    "        if stat_single.size == 0 or stat_close.size == 0:\n",
    "            print(f\"\\nPair:\\n  Single: {single_path}\\n  Social: {social_path}\")\n",
    "            print(\"  单侧或社交-close 数据为空，跳过此对。\")\n",
    "            continue\n",
    "\n",
    "        # 4.3 Mann–Whitney U 检验 (95th ΔF/F)\n",
    "        u_stat_df, p_value_df = mannwhitneyu(stat_single, stat_close, alternative='two-sided')\n",
    "\n",
    "        # 4.4 计算 firing frequency\n",
    "        _, freq_single = get_firing_frequency(single_path, exclude_dict)\n",
    "        _, freq_social = get_firing_frequency(social_path, exclude_dict)\n",
    "        # 如果某一侧频率为空，也跳过该检验\n",
    "        if freq_single.size == 0 or freq_social.size == 0:\n",
    "            freq_test_pass = False\n",
    "        else:\n",
    "            freq_test_pass = True\n",
    "            u_stat_freq, p_value_freq = mannwhitneyu(freq_single, freq_social, alternative='two-sided')\n",
    "\n",
    "        # 4.5 打印结果\n",
    "        print(f\"\\n===== 会话对 比较（不匹配 ROI 名）=====\")\n",
    "        print(f\"Single 会话: {single_path}\")\n",
    "        print(f\"Social-close 会话: {social_path}\")\n",
    "\n",
    "        # 95th ΔF/F 的描述性统计\n",
    "        mean_sing = np.nanmean(stat_single)\n",
    "        std_sing  = np.nanstd(stat_single)\n",
    "        med_sing  = np.nanmedian(stat_single)\n",
    "        n_sing    = np.count_nonzero(~np.isnan(stat_single))\n",
    "\n",
    "        mean_soc = np.nanmean(stat_close)\n",
    "        std_soc  = np.nanstd(stat_close)\n",
    "        med_soc  = np.nanmedian(stat_close)\n",
    "        n_soc    = np.count_nonzero(~np.isnan(stat_close))\n",
    "\n",
    "        print(f\"\\n--- 95th-% ΔF/F 分布（Single vs Social-close）---\")\n",
    "        print(f\"Single:       n={n_sing}, mean={mean_sing:.3f}, std={std_sing:.3f}, median={med_sing:.3f}\")\n",
    "        print(f\"Social-close: n={n_soc}, mean={mean_soc:.3f}, std={std_soc:.3f}, median={med_soc:.3f}\")\n",
    "        print(f\"Mann–Whitney U (ΔF/F): U={u_stat_df:.3f}, p-value={p_value_df:.5f}\")\n",
    "\n",
    "        # 频率的描述性统计\n",
    "        if freq_test_pass:\n",
    "            mean_fs = np.nanmean(freq_single)\n",
    "            std_fs  = np.nanstd(freq_single)\n",
    "            med_fs  = np.nanmedian(freq_single)\n",
    "            n_fs    = np.count_nonzero(~np.isnan(freq_single))\n",
    "\n",
    "            mean_fc = np.nanmean(freq_social)\n",
    "            std_fc  = np.nanstd(freq_social)\n",
    "            med_fc  = np.nanmedian(freq_social)\n",
    "            n_fc    = np.count_nonzero(~np.isnan(freq_social))\n",
    "\n",
    "            print(f\"\\n--- firing frequency (Hz) 分布（Single vs Social）---\")\n",
    "            print(f\"Single freq:       n={n_fs}, mean={mean_fs:.3f}, std={std_fs:.3f}, median={med_fs:.3f}\")\n",
    "            print(f\"Social freq:       n={n_fc}, mean={mean_fc:.3f}, std={std_fc:.3f}, median={med_fc:.3f}\")\n",
    "            print(f\"Mann–Whitney U (freq): U={u_stat_freq:.3f}, p-value={p_value_freq:.5f}\")\n",
    "        else:\n",
    "            print(\"\\n无法比较 firing frequency：某一侧频率数据为空。\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5. 对整体 V1 与 PMC 会话做描述性统计和非配对检验\n",
    "# ----------------------------------------------------------------------------\n",
    "def compute_and_print_stats(label, all_far, all_close, all_single):\n",
    "    \"\"\"\n",
    "    1. 对 Social Far vs Social Close 做配对的 Wilcoxon signed-rank 检验\n",
    "    2. 对 Single vs Social Close 做 Mann-Whitney U 检验（不配对）\n",
    "    3. 对 Single vs Social Far   做 Mann-Whitney U 检验（不配对）\n",
    "    打印均值、标准差、中位数和 p 值。\n",
    "    \"\"\"\n",
    "    def desc_stats(arr):\n",
    "        return {\n",
    "            'mean': np.nanmean(arr),\n",
    "            'std':  np.nanstd(arr),\n",
    "            'median': np.nanmedian(arr),\n",
    "            'n':    np.count_nonzero(~np.isnan(arr))\n",
    "        }\n",
    "\n",
    "    stats_far    = desc_stats(all_far)\n",
    "    stats_close  = desc_stats(all_close)\n",
    "    stats_single = desc_stats(all_single)\n",
    "\n",
    "    print(f\"\\n===== {label} =====\")\n",
    "    print(f\"Social-Far:    n={stats_far['n']}, mean={stats_far['mean']:.3f}, std={stats_far['std']:.3f}, median={stats_far['median']:.3f}\")\n",
    "    print(f\"Social-Close:  n={stats_close['n']}, mean={stats_close['mean']:.3f}, std={stats_close['std']:.3f}, median={stats_close['median']:.3f}\")\n",
    "    print(f\"Single:        n={stats_single['n']}, mean={stats_single['mean']:.3f}, std={stats_single['std']:.3f}, median={stats_single['median']:.3f}\")\n",
    "\n",
    "    # 1. 配对 Wilcoxon：Social-Far vs Social-Close\n",
    "    if len(all_far) == len(all_close) and len(all_far) > 0:\n",
    "        w_stat, w_p = wilcoxon(all_far, all_close)\n",
    "        print(f\"\\nPaired Wilcoxon (Far vs Close): statistic={w_stat:.3f}, p-value={w_p:.5f}\")\n",
    "    else:\n",
    "        print(\"\\nPaired Wilcoxon (Far vs Close): 数据长度不匹配或无有效数据，无法进行配对检验。\")\n",
    "\n",
    "    # 2. Single vs Social-Close（Mann-Whitney U，不配对）\n",
    "    if len(all_single) > 0 and len(all_close) > 0:\n",
    "        u_stat_sc, u_p_sc = mannwhitneyu(all_single, all_close, alternative='two-sided')\n",
    "        print(f\"Mann-Whitney U (Single vs Close): U={u_stat_sc:.3f}, p-value={u_p_sc:.5f}\")\n",
    "    else:\n",
    "        print(\"Mann-Whitney U (Single vs Close): 数据不足，无法进行检验。\")\n",
    "\n",
    "    # 3. Single vs Social-Far（Mann-Whitney U，不配对）\n",
    "    if len(all_single) > 0 and len(all_far) > 0:\n",
    "        u_stat_sf, u_p_sf = mannwhitneyu(all_single, all_far, alternative='two-sided')\n",
    "        print(f\"Mann-Whitney U (Single vs Far): U={u_stat_sf:.3f}, p-value={u_p_sf:.5f}\")\n",
    "    else:\n",
    "        print(\"Mann-Whitney U (Single vs Far): 数据不足，无法进行检验。\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6. 会话路径示例（请根据实际情况修改）\n",
    "# ----------------------------------------------------------------------------\n",
    "pmc_social_sessions = [\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_27/20241015PMCBE1mini_p20241015PMCRE1_12_33\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_05_16/20240303PMCBE0r1coatedmini_p20240303RE1\",\n",
    "]\n",
    "\n",
    "pmc_single_sessions = [\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_24/20241001PMCr2_16_19\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_25/20241002PMCr2_16_25\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_06/20241015pmcr2_16_53\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_06/20241015pmcr2_17_13\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_12/20241001PMCRE2mini_13_44\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_12/20241001PMCRE2mini_13_57\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_12/20241001PMCRE2mini_15_35\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_13/20241225PMCLE1mini_11_06\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_27/20241015PMCBE1mini_12_24\"\n",
    "]\n",
    "\n",
    "v1_social_sessions = [\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_18/20240919v1l5r1mini_p20240717PMC_social_test_11_30\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_31/20240919v1l5r2mini_p20240717PMC_social_14_04\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_31/2social_mini_20240819V1r1_single_11_29\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_31/2social_mini_20240819V1r1_femalebleach_11_48\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_05_16/20241216V1RE1Fmini_p20241216RE2\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2025_05_16/20241216V1RE1Fmini_p20241224PMCLE1\",\n",
    "]\n",
    "\n",
    "v1_single_sessions = [\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r1_16_37\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r1_16_53\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r2_14_30\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r2_15_58\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_17/20240819V1r1_13_41\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_17/20240819V1r1_14_25\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21\",\n",
    "    \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54\",\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7. V1 指定的两对会话 (single, social-close)，不做 ROI 匹配\n",
    "# ----------------------------------------------------------------------------\n",
    "v1_paired_sessions = [\n",
    "    (\n",
    "        \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21\",\n",
    "        \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_18/20240919v1l5r1mini_p20240717PMC_social_test_11_30\"\n",
    "    ),\n",
    "    (\n",
    "        \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54\",\n",
    "        \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_31/20240919v1l5r2mini_p20240717PMC_social_14_04\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8. 主流程\n",
    "# ----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    threshold = 250.0  # COM 距离阈值\n",
    "    pct = 95           # 使用第 95 百分位数\n",
    "\n",
    "    # # --- PMC 整体统计 ---\n",
    "    # print(\"=== PMC 整体统计 ===\")\n",
    "    # pmc_far, pmc_close = aggregate_social_far_close(pmc_social_sessions, exclude_mapping, threshold, pct)\n",
    "    # pmc_single        = aggregate_single_stats(pmc_single_sessions, exclude_mapping, pct)\n",
    "    # compute_and_print_stats(\"PMC (Social vs Single)\", pmc_far, pmc_close, pmc_single)\n",
    "\n",
    "    # # --- V1 整体统计 ---\n",
    "    # print(\"\\n=== V1 整体统计 ===\")\n",
    "    # v1_far, v1_close  = aggregate_social_far_close(v1_social_sessions, exclude_mapping, threshold, pct)\n",
    "    # v1_single         = aggregate_single_stats(v1_single_sessions, exclude_mapping, pct)\n",
    "    # compute_and_print_stats(\"V1 (Social vs Single)\", v1_far, v1_close, v1_single)\n",
    "\n",
    "    # --- V1 指定会话对（不匹配 ROI）---\n",
    "    print(\"\\n=== V1 指定会话对比较（不匹配 ROI 名称） ===\")\n",
    "    compute_session_pair_unpaired(v1_paired_sessions, exclude_mapping, pct)\n",
    "\n",
    "    print(\"\\n所有统计计算完毕。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbop241209",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
