{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r1_16_37/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r1_16_53/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r2_14_30/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r2_15_58/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_17/20240819V1r1_13_41/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max25_diff3.5_pnr1.1.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_17/20240819V1r1_14_25/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1000_stp700_max25_diff3.5_pnr1.1.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_24/20241001PMCr2_16_19/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max15_diff3.5_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_25/20241002PMCr2_16_25/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max15_diff3.5_pnr1.1.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_01/20240819V1r1_AO_14_56/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max25_diff5.0_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_01/20240910V1r_AO_12_50/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_06/20241015pmcr2_16_53/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max15_diff3.5_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_06/20241015pmcr2_17_13/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1000_stp700_max15_diff3.5_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_07/20241015pmcr2_AO_12_52/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max15_diff5.0_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd700_stp700_max25_diff5.0_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max25_diff5.0_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_12/20241001PMCRE2mini_13_44/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max15_diff5.0_pnr1.1.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_12/20241001PMCRE2mini_13_57/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max15_diff4.0_pnr1.1.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_12/20241001PMCRE2mini_15_35/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max15_diff4.0_pnr1.1.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_13/20241225PMCLE1mini_11_06/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max15_diff3.5_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2025_02_27/20241015PMCBE1mini_12_24/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd700_stp700_max15_diff3.5_pnr1.1.h5\n",
      "Loaded 20 sessions.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "\n",
    "# def load_session_data(rec_path):\n",
    "#     \"\"\"\n",
    "#     Load a single session's HDF5 file by dynamically searching the MIR_Aligned folder.\n",
    "\n",
    "#     Selection logic:\n",
    "#       1) If exactly one file matches aligned_predictions_with_ca_and_dF_F*.h5, use it.\n",
    "#       2) Otherwise, pick the file with the longest stem (most extra info).\n",
    "#     \"\"\"\n",
    "#     rec_path = Path(rec_path)\n",
    "#     aligned_dir = rec_path / \"MIR_Aligned\"\n",
    "\n",
    "#     # find all matching .h5 files\n",
    "#     h5_paths = list(aligned_dir.glob(\"aligned_predictions_with_ca_and_dF_F*.h5\"))\n",
    "#     if not h5_paths:\n",
    "#         raise FileNotFoundError(f\"No .h5 files found in {aligned_dir}\")\n",
    "\n",
    "#     # selection rule\n",
    "#     if len(h5_paths) == 1:\n",
    "#         hdf5_file_path = h5_paths[0]\n",
    "#     else:\n",
    "#         # choose the file whose stem has the most characters\n",
    "#         hdf5_file_path = max(h5_paths, key=lambda p: len(p.stem))\n",
    "\n",
    "#     print(\"Using:\", hdf5_file_path)\n",
    "\n",
    "#     # load dataframe\n",
    "#     df = pd.read_hdf(hdf5_file_path, key='df')\n",
    "\n",
    "#     # extract metadata\n",
    "#     session_id      = rec_path.name\n",
    "#     recording_date  = rec_path.parent.name\n",
    "#     experiment_name = rec_path.parent.parent.name\n",
    "\n",
    "#     df['session_id']     = session_id\n",
    "#     df['recording_date'] = recording_date\n",
    "#     df['experiment']     = experiment_name\n",
    "#     df['session_path']   = str(rec_path)\n",
    "#     df['file_path']      = str(hdf5_file_path)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def load_sessions_from_csv(csv_filepath, base_path, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Load session data from a CSV file containing relative session paths.\n",
    "    \n",
    "#     This function assumes that the CSV file contains a column (by default, it will look for \n",
    "#     a column named 'relative_path'; if not found, it will use the first column) that lists the \n",
    "#     relative paths for each session. The provided base path is then prepended to each relative path \n",
    "#     to form the full session directory. Finally, it uses your preexisting function load_session_data \n",
    "#     to load each session's data.\n",
    "    \n",
    "#     Parameters:\n",
    "#       - csv_filepath (str): The path to the CSV file containing the relative session paths.\n",
    "#       - base_path (str): The base path to be prepended to each relative path.\n",
    "#       - verbose (bool): If True, print messages when a session fails to load.\n",
    "    \n",
    "#     Returns:\n",
    "#       - sessions (list): A list of DataFrames, one for each successfully loaded session.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         df_paths = pd.read_csv(csv_filepath)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading CSV file at {csv_filepath}: {e}\")\n",
    "#         return []\n",
    "    \n",
    "#     # Determine which column contains the relative paths. Look for a column named 'relative_path'\n",
    "#     # otherwise default to the first column.\n",
    "#     if 'relative_path' in df_paths.columns:\n",
    "#         relative_paths = df_paths['relative_path'].tolist()\n",
    "#     else:\n",
    "#         relative_paths = df_paths.iloc[:, 0].tolist()\n",
    "    \n",
    "#     sessions = []\n",
    "#     for rel_path in relative_paths:\n",
    "#         # Build the full session path using the base path\n",
    "#         session_path = os.path.join(base_path, rel_path)\n",
    "#         try:\n",
    "#             df_session = load_session_data(session_path)\n",
    "#             sessions.append(df_session)\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(f\"Could not load session at {session_path}: {e}\")\n",
    "#     return sessions\n",
    "\n",
    "# # =============================================================================\n",
    "# # Example Usage:\n",
    "# # =============================================================================\n",
    "# # Set the base path that will be prepended to each relative path.\n",
    "# base_path = \"/data/big_rim/rsync_dcc_sum/Oct3V1\" #\"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1\"\n",
    "\n",
    "# # CSV file that contains the relative session paths.\n",
    "# csv_file = \"/home/lq53/mir_repos/BBOP/random_tests/25mar_minibbop_integration/250331_sum_aligned_good_path_relative.csv\" #\"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/2504_mir_loader/250331_sum_aligned_good_path_relative.csv\"\n",
    "# # Load all sessions\n",
    "# all_sessions = load_sessions_from_csv(csv_file, base_path)\n",
    "# print(f\"Loaded {len(all_sessions)} sessions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r1_16_37/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r1_16_53/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r2_14_30/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_14/20240916v1r2_15_58/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_17/20240819V1r1_13_41/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max25_diff3.5_pnr1.1.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_17/20240819V1r1_14_25/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1000_stp700_max25_diff3.5_pnr1.1.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_01/20240819V1r1_AO_14_56/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max25_diff5.0_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_01/20240910V1r_AO_12_50/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd700_stp700_max25_diff5.0_pnrauto.h5\n",
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1500_stp700_max25_diff5.0_pnrauto.h5\n",
      "Loaded 10 v1 sessions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_session_data(rec_path):\n",
    "    \"\"\"\n",
    "    Load a single session's HDF5 file by dynamically searching the MIR_Aligned folder.\n",
    "\n",
    "    Selection logic:\n",
    "      1) If exactly one file matches aligned_predictions_with_ca_and_dF_F*.h5, use it.\n",
    "      2) Otherwise, pick the file with the longest stem (most extra info).\n",
    "    \"\"\"\n",
    "    rec_path = Path(rec_path)\n",
    "    aligned_dir = rec_path / \"MIR_Aligned\"\n",
    "\n",
    "    # find all matching .h5 files\n",
    "    h5_paths = list(aligned_dir.glob(\"aligned_predictions_with_ca_and_dF_F*.h5\"))\n",
    "    if not h5_paths:\n",
    "        raise FileNotFoundError(f\"No .h5 files found in {aligned_dir}\")\n",
    "\n",
    "    # selection rule\n",
    "    if len(h5_paths) == 1:\n",
    "        hdf5_file_path = h5_paths[0]\n",
    "    else:\n",
    "        # choose the file whose stem has the most characters\n",
    "        hdf5_file_path = max(h5_paths, key=lambda p: len(p.stem))\n",
    "\n",
    "    print(\"Using:\", hdf5_file_path)\n",
    "\n",
    "    # load dataframe\n",
    "    df = pd.read_hdf(hdf5_file_path, key='df')\n",
    "\n",
    "    # extract metadata\n",
    "    session_id      = rec_path.name\n",
    "    recording_date  = rec_path.parent.name\n",
    "    experiment_name = rec_path.parent.parent.name\n",
    "\n",
    "    df['session_id']     = session_id\n",
    "    df['recording_date'] = recording_date\n",
    "    df['experiment']     = experiment_name\n",
    "    df['session_path']   = str(rec_path)\n",
    "    df['file_path']      = str(hdf5_file_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_sessions_from_csv(\n",
    "    csv_filepath,\n",
    "    base_path,\n",
    "    filter_fn=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Load session data from a CSV file of relative paths, with optional dynamic filtering.\n",
    "\n",
    "    Params:\n",
    "      - csv_filepath (str): path to the CSV file (must contain a column 'relative_path' or use first column)\n",
    "      - base_path (str): prefix for each relative path\n",
    "      - filter_fn (callable, optional): function taking a relative_path string and returning True if\n",
    "        that session should be loaded, False to skip it. If None, all are loaded.\n",
    "      - verbose (bool): print failures\n",
    "\n",
    "    Returns:\n",
    "      - List[pd.DataFrame]: one DataFrame per session that passed filter_fn and loaded successfully.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_paths = pd.read_csv(csv_filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file at {csv_filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # determine which column contains relative paths\n",
    "    if 'relative_path' in df_paths.columns:\n",
    "        relative_paths = df_paths['relative_path'].tolist()\n",
    "    else:\n",
    "        relative_paths = df_paths.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    # apply dynamic filter if provided\n",
    "    if filter_fn is not None:\n",
    "        relative_paths = [rp for rp in relative_paths if filter_fn(rp)]\n",
    "\n",
    "    sessions = []\n",
    "    for rel_path in relative_paths:\n",
    "        session_path = Path(base_path) / rel_path\n",
    "        try:\n",
    "            df_session = load_session_data(session_path)\n",
    "            sessions.append(df_session)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Could not load session at {session_path}: {e}\")\n",
    "    return sessions\n",
    "\n",
    "\n",
    "\n",
    "# === Example usage ===\n",
    "base_path = \"/data/big_rim/rsync_dcc_sum/Oct3V1\"\n",
    "csv_file = \"/home/lq53/mir_repos/BBOP/random_tests/25mar_minibbop_integration/250331_sum_aligned_good_path_relative.csv\" #\"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/2504_mir_loader/250331_sum_aligned_good_path_relative.csv\"\n",
    "\n",
    "\n",
    "# Load only sessions whose path contains 'v1'\n",
    "v1_sessions = load_sessions_from_csv(\n",
    "    csv_file,\n",
    "    base_path,\n",
    "    filter_fn=lambda rp: 'v1' in rp.lower()\n",
    ")\n",
    "print(f\"Loaded {len(v1_sessions)} v1 sessions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H5 files: [PosixPath('/data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_06/20241015pmcr2_17_13/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1000_stp700_max15_diff3.5_pnrauto.h5')]\n",
      "['/df']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# --- Load prediction H5 and compute head frames ---\n",
    "base_path = \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_06/20241015pmcr2_17_13\"\n",
    "aligned_dir = Path(base_path) / 'MIR_Aligned'\n",
    "h5_paths = list(aligned_dir.glob('aligned_predictions_with_ca_and_dF_F_*.h5'))\n",
    "if not h5_paths:\n",
    "    raise FileNotFoundError(f\"No .h5 files found in {aligned_dir}\")\n",
    "print(\"H5 files:\", h5_paths)\n",
    "\n",
    "h5 = h5_paths[0]\n",
    "with pd.HDFStore(h5) as store:\n",
    "    print(store.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: /data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_06/20241015pmcr2_17_13/MIR_Aligned/aligned_predictions_with_ca_and_dF_F_wnd1000_stp700_max15_diff3.5_pnrauto.h5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(\"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_11_06/20241015pmcr2_17_13\")\n",
    "aligned_dir = base_path / \"MIR_Aligned\"\n",
    "\n",
    "# find all .h5 files matching the base pattern (with or without suffix)\n",
    "h5_paths = list(aligned_dir.glob(\"aligned_predictions_with_ca_and_dF_F*.h5\"))\n",
    "if not h5_paths:\n",
    "    raise FileNotFoundError(f\"No .h5 files found in {aligned_dir}\")\n",
    "\n",
    "# classify exact vs. suffix variants\n",
    "exact_name  = \"aligned_predictions_with_ca_and_dF_F.h5\"\n",
    "exact_files = [p for p in h5_paths if p.name == exact_name]\n",
    "suffix_files = [p for p in h5_paths if p.name != exact_name]\n",
    "\n",
    "# decision rules:\n",
    "# 1) If only the exact file exists, use it.\n",
    "# 2) Otherwise, if any suffix file exists, pick the one with the longest name\n",
    "#    (i.e. the \"more things\" variant).\n",
    "# 3) Fallback: first match.\n",
    "if exact_files and not suffix_files:\n",
    "    hdf5_file_path = exact_files[0]\n",
    "elif suffix_files:\n",
    "    hdf5_file_path = max(suffix_files, key=lambda p: len(p.stem))\n",
    "else:\n",
    "    hdf5_file_path = h5_paths[0]\n",
    "\n",
    "print(\"Using:\", hdf5_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loader function that i shared with all.\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# # load_session_data is a function from BBOP, but copy pasted here:\n",
    "# def load_session_data(rec_path):\n",
    "#     \"\"\"\n",
    "#     Load a single session's HDF5 file by dynamically searching the MIR_Aligned folder.\n",
    "    \n",
    "#     If only one HDF5 file is present (typically named exactly\n",
    "#     'aligned_predictions_with_ca_and_dF_F.h5'), it is used.\n",
    "    \n",
    "#     If more than one file exists, the function will look for one that contains\n",
    "#     additional identifying information (e.g., with 'wnd1500' in the filename).\n",
    "#     If found, that file is used; otherwise, the first file is chosen.\n",
    "\n",
    "#     The function extracts metadata from the folder structure and appends it\n",
    "#     to the DataFrame.\n",
    "#     \"\"\"\n",
    "#     mir_aligned_path = os.path.join(rec_path, 'MIR_Aligned')\n",
    "#     # Search for all HDF5 files in the MIR_Aligned folder\n",
    "#     hdf5_files = glob.glob(os.path.join(mir_aligned_path, '*.h5'))\n",
    "#     if not hdf5_files:\n",
    "#         raise FileNotFoundError(f\"No HDF5 files found in {mir_aligned_path}\")\n",
    "#     if len(hdf5_files) == 1:\n",
    "#         hdf5_file_path = hdf5_files[0]\n",
    "#     else:\n",
    "#         # Look for a file that contains 'wnd1500' in its name\n",
    "#         matching_files = [f for f in hdf5_files if 'wnd1500' in os.path.basename(f)]\n",
    "#         if matching_files:\n",
    "#             hdf5_file_path = matching_files[0]\n",
    "#         else:\n",
    "#             hdf5_file_path = hdf5_files[0]  # fallback to first file if no match found\n",
    "\n",
    "#     df = pd.read_hdf(hdf5_file_path, key='df')\n",
    "    \n",
    "#     # Extract metadata based on the folder structure:\n",
    "#     # e.g., /data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_25/20241002PMCr2_15_42\n",
    "#     norm_path = os.path.normpath(rec_path)\n",
    "#     session_id = os.path.basename(norm_path)\n",
    "#     recording_date = os.path.basename(os.path.dirname(norm_path))\n",
    "#     experiment_name = os.path.basename(os.path.dirname(os.path.dirname(norm_path)))\n",
    "    \n",
    "#     df['session_id'] = session_id\n",
    "#     df['recording_date'] = recording_date\n",
    "#     df['experiment'] = experiment_name\n",
    "#     df['session_path'] = rec_path\n",
    "#     df['file_path'] = hdf5_file_path\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def load_sessions_from_csv(csv_filepath, base_path, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Load session data from a CSV file containing relative session paths.\n",
    "    \n",
    "#     This function assumes that the CSV file contains a column (by default, it will look for \n",
    "#     a column named 'relative_path'; if not found, it will use the first column) that lists the \n",
    "#     relative paths for each session. The provided base path is then prepended to each relative path \n",
    "#     to form the full session directory. Finally, it uses your preexisting function load_session_data \n",
    "#     to load each session's data.\n",
    "    \n",
    "#     Parameters:\n",
    "#       - csv_filepath (str): The path to the CSV file containing the relative session paths.\n",
    "#       - base_path (str): The base path to be prepended to each relative path.\n",
    "#       - verbose (bool): If True, print messages when a session fails to load.\n",
    "    \n",
    "#     Returns:\n",
    "#       - sessions (list): A list of DataFrames, one for each successfully loaded session.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         df_paths = pd.read_csv(csv_filepath)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading CSV file at {csv_filepath}: {e}\")\n",
    "#         return []\n",
    "    \n",
    "#     # Determine which column contains the relative paths. Look for a column named 'relative_path'\n",
    "#     # otherwise default to the first column.\n",
    "#     if 'relative_path' in df_paths.columns:\n",
    "#         relative_paths = df_paths['relative_path'].tolist()\n",
    "#     else:\n",
    "#         relative_paths = df_paths.iloc[:, 0].tolist()\n",
    "    \n",
    "#     sessions = []\n",
    "#     for rel_path in relative_paths:\n",
    "#         # Build the full session path using the base path\n",
    "#         session_path = os.path.join(base_path, rel_path)\n",
    "#         try:\n",
    "#             df_session = load_session_data(session_path)\n",
    "#             sessions.append(df_session)\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(f\"Could not load session at {session_path}: {e}\")\n",
    "#     return sessions\n",
    "\n",
    "# # =============================================================================\n",
    "# # Example Usage:\n",
    "# # =============================================================================\n",
    "# # Set the base path that will be prepended to each relative path.\n",
    "# base_path = \"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1\"\n",
    "\n",
    "# # CSV file that contains the relative session paths.\n",
    "# csv_file = \"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/2504_mir_loader/250331_sum_aligned_good_path_relative.csv\"\n",
    "\n",
    "# # Load all sessions\n",
    "# all_sessions = load_sessions_from_csv(csv_file, base_path)\n",
    "# print(f\"Loaded {len(all_sessions)} sessions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbop241209",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
