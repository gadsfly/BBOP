{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# this method is changing shape of dataset, at least the dappy features.. should not use.\n",
    "# # dappy env\n",
    "# import json\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# def export_aligned_data_to_h5(\n",
    "#     data_obj, \n",
    "#     rec_path, \n",
    "#     frame_mapping_file, \n",
    "#     out_file\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Filters data_obj to a specific rec_path and frames from frame_mapping_file,\n",
    "#     then saves minimal data to an HDF5 file (with string columns stored\n",
    "#     as variable-length UTF-8 strings).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 1. Filter by Prediction_path\n",
    "#     rec_mat_path = os.path.join(rec_path, \"DANNCE/predict00/save_data_AVG.mat\")\n",
    "#     path_data = data_obj.data[data_obj.data[\"Prediction_path\"] == rec_mat_path].copy()\n",
    "#     if path_data.empty:\n",
    "#         print(f\"No data found for rec_path: {rec_mat_path}\")\n",
    "#         return\n",
    "    \n",
    "\n",
    "#     # 2. Read the frame mapping JSON\n",
    "#     with open(frame_mapping_file, \"r\") as f:\n",
    "#         map_data = json.load(f)\n",
    "#     mapped_frames = set(map_data[\"mapped_sixcam_frame_indices\"])\n",
    "#     time_offset = map_data[\"time_offset\"]\n",
    "    \n",
    "#     # 3. Filter rows by mapped frame indices (assuming your DataFrame has 'frame' column)\n",
    "#     if \"frame\" not in path_data.columns:\n",
    "#         raise ValueError(\"DataFrame does not have 'frame' column to filter by.\")\n",
    "    \n",
    "#     # Adjust frames so that they start at 0\n",
    "#     min_frame = path_data[\"frame\"].min()\n",
    "#     path_data[\"frame\"] = path_data[\"frame\"] - min_frame\n",
    "\n",
    "#     # 1. Create helper offsets DataFrame\n",
    "#     offsets = pd.DataFrame({'offset': range(10)})\n",
    "\n",
    "#     # 2. Cross-merge (pandas 1.2+ supports `how=\"cross\"`)\n",
    "#     expanded = path_data.merge(offsets, how='cross')\n",
    "\n",
    "#     # 3. Update the frame by adding the offset\n",
    "#     expanded['frame'] = expanded['frame'] + expanded['offset']\n",
    "#     expanded.drop(columns='offset', inplace=True)\n",
    "    \n",
    "#     filtered_data = expanded[expanded[\"frame\"].isin(mapped_frames)]\n",
    "    \n",
    "#     if filtered_data.empty:\n",
    "#         print(\"No overlapping frames found between path_data and mapped_sixcam_frame_indices.\")\n",
    "    \n",
    "#     # 4. Save to HDF5\n",
    "#     os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "#     # Use variable-length string dtype for columns that are (or become) text\n",
    "#     variable_length_string_dt = h5py.special_dtype(vlen=str)\n",
    "    \n",
    "#     with h5py.File(out_file, \"w\") as hf:\n",
    "        \n",
    "#         #\n",
    "#         # (A) Save filtered DataFrame columns\n",
    "#         #\n",
    "#         grp = hf.create_group(\"filtered_data\")\n",
    "        \n",
    "#         for col in filtered_data.columns:\n",
    "#             col_data = filtered_data[col].to_numpy()  # get as NumPy array\n",
    "            \n",
    "#             # Check if it's string-like (object, unicode, or bytes)\n",
    "#             if col_data.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "#                 # Convert each element to a Python string, store as variable-length UTF-8\n",
    "#                 # Flatten, map to str, then reshape to original shape if multi-dimensional\n",
    "#                 original_shape = col_data.shape\n",
    "#                 col_data = col_data.reshape(-1)  # flatten\n",
    "#                 col_data = np.array([str(item) for item in col_data], dtype=object)\n",
    "#                 col_data = col_data.reshape(original_shape)\n",
    "                \n",
    "#                 grp.create_dataset(\n",
    "#                     col, \n",
    "#                     data=col_data, \n",
    "#                     dtype=variable_length_string_dt, \n",
    "#                     compression=\"gzip\"\n",
    "#                 )\n",
    "#             else:\n",
    "#                 # Numeric or other supported dtype can be written directly\n",
    "#                 grp.create_dataset(col, data=col_data, compression=\"gzip\")\n",
    "        \n",
    "#         #\n",
    "#         # (B) Save relevant data_obj attributes\n",
    "#         #\n",
    "#         if hasattr(data_obj, \"embed_vals\") and data_obj.embed_vals is not None:\n",
    "#             hf.create_dataset(\"embed_vals\", data=data_obj.embed_vals, compression=\"gzip\")\n",
    "        \n",
    "#         # Example: saving meta info\n",
    "#         if hasattr(data_obj, \"meta\") and data_obj.meta is not None:\n",
    "#             meta_grp = hf.create_group(\"meta\")\n",
    "#             if isinstance(data_obj.meta, dict):\n",
    "#                 for key, val in data_obj.meta.items():\n",
    "#                     # Convert to array for consistency\n",
    "#                     val_array = np.array(val, dtype=object)  # object to handle strings\n",
    "#                     # If it has any string/unicode, cast them properly\n",
    "#                     if val_array.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "#                         val_array = val_array.reshape(-1)\n",
    "#                         val_array = np.array([str(item) for item in val_array], dtype=object)\n",
    "#                         # Reshape back if needed (only if it's consistent)\n",
    "#                         # But typically meta might be a 1D list, so might not need reshape\n",
    "                        \n",
    "#                         meta_grp.create_dataset(\n",
    "#                             key,\n",
    "#                             data=val_array,\n",
    "#                             dtype=variable_length_string_dt,\n",
    "#                             compression=\"gzip\"\n",
    "#                         )\n",
    "#                     else:\n",
    "#                         meta_grp.create_dataset(key, data=val_array, compression=\"gzip\")\n",
    "#             else:\n",
    "#                 # Non-dict meta structure\n",
    "#                 meta_vals = np.array(data_obj.meta, dtype=object)\n",
    "#                 if meta_vals.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "#                     meta_vals = [str(item) for item in meta_vals.flatten()]\n",
    "#                     meta_vals = np.array(meta_vals, dtype=object)\n",
    "#                     meta_grp.create_dataset(\n",
    "#                         \"meta_data\",\n",
    "#                         data=meta_vals,\n",
    "#                         dtype=variable_length_string_dt,\n",
    "#                         compression=\"gzip\"\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     meta_grp.create_dataset(\"meta_data\", data=meta_vals, compression=\"gzip\")\n",
    "        \n",
    "        \n",
    "#         # (C) Save frame mapping info\n",
    "#         #\n",
    "#         map_grp = hf.create_group(\"frame_mapping\")\n",
    "#         map_grp.create_dataset(\n",
    "#             \"mapped_sixcam_frame_indices\",\n",
    "#             data=np.array(map_data[\"mapped_sixcam_frame_indices\"]),\n",
    "#             compression=\"gzip\"\n",
    "#         )\n",
    "#         map_grp.attrs[\"time_offset\"] = time_offset\n",
    "\n",
    "#     print(f\"Filtered data for '{rec_path}' saved to '{out_file}'\")\n",
    "\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle  # used for pickling non-uniform arrays\n",
    "\n",
    "def export_aligned_data_to_h5(\n",
    "    data_obj, \n",
    "    rec_path, \n",
    "    frame_mapping_file, \n",
    "    out_file\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters data_obj to a specific rec_path and frames from frame_mapping_file,\n",
    "    then saves minimal data to an HDF5 file.\n",
    "    \n",
    "    This version preserves the shape of any np.ndarray stored in a DataFrame column.\n",
    "    For columns with ndarray entries:\n",
    "      - If all arrays have the same shape, they are stacked and saved directly.\n",
    "      - If they vary in shape, each array is pickled and stored as a binary blob.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Filter by Prediction_path\n",
    "    rec_mat_path = os.path.join(rec_path, \"DANNCE/predict00/save_data_AVG.mat\")\n",
    "    path_data = data_obj.data[data_obj.data[\"Prediction_path\"] == rec_mat_path].copy()\n",
    "    if path_data.empty:\n",
    "        print(f\"No data found for rec_path: {rec_mat_path}\")\n",
    "        return\n",
    "\n",
    "    # 2. Read the frame mapping JSON\n",
    "    with open(frame_mapping_file, \"r\") as f:\n",
    "        map_data = json.load(f)\n",
    "    mapped_frames = set(map_data[\"mapped_sixcam_frame_indices\"])\n",
    "    time_offset = map_data[\"time_offset\"]\n",
    "    \n",
    "    # 3. Filter rows by mapped frame indices (assuming your DataFrame has a 'frame' column)\n",
    "    if \"frame\" not in path_data.columns:\n",
    "        raise ValueError(\"DataFrame does not have 'frame' column to filter by.\")\n",
    "    \n",
    "    # Adjust frames so that they start at 0\n",
    "    min_frame = path_data[\"frame\"].min()\n",
    "    path_data[\"frame\"] = path_data[\"frame\"] - min_frame\n",
    "\n",
    "    # Create helper offsets DataFrame\n",
    "    offsets = pd.DataFrame({'offset': range(10)})\n",
    "\n",
    "    # Cross-merge (pandas 1.2+ supports `how=\"cross\"`)\n",
    "    expanded = path_data.merge(offsets, how='cross')\n",
    "\n",
    "    # Update the frame by adding the offset\n",
    "    expanded['frame'] = expanded['frame'] + expanded['offset']\n",
    "    expanded.drop(columns='offset', inplace=True)\n",
    "    \n",
    "    filtered_data = expanded[expanded[\"frame\"].isin(mapped_frames)]\n",
    "    \n",
    "    if filtered_data.empty:\n",
    "        print(\"No overlapping frames found between path_data and mapped_sixcam_frame_indices.\")\n",
    "    \n",
    "    # 4. Save to HDF5\n",
    "    os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "    \n",
    "    with h5py.File(out_file, \"w\") as hf:\n",
    "        \n",
    "        # (A) Save filtered DataFrame columns\n",
    "        grp = hf.create_group(\"filtered_data\")\n",
    "        \n",
    "        for col in filtered_data.columns:\n",
    "            col_data = filtered_data[col].to_numpy()\n",
    "            \n",
    "            # If the first element is an ndarray, handle it specially.\n",
    "            if len(col_data) > 0 and isinstance(col_data[0], np.ndarray):\n",
    "                try:\n",
    "                    # Try stacking arrays if they have the same shape\n",
    "                    stacked = np.stack(col_data)\n",
    "                    grp.create_dataset(col, data=stacked, compression=\"gzip\")\n",
    "                except ValueError:\n",
    "                    # For non-uniform shapes, pickle each array so that its structure is preserved.\n",
    "                    binary_dtype = h5py.special_dtype(vlen=bytes)\n",
    "                    pickled_data = np.array([pickle.dumps(item) for item in col_data])\n",
    "                    grp.create_dataset(col, data=pickled_data, dtype=binary_dtype, compression=\"gzip\")\n",
    "            \n",
    "            # Handle string/text columns\n",
    "            elif col_data.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "                variable_length_string_dt = h5py.special_dtype(vlen=str)\n",
    "                # Convert each element explicitly to a Python string to avoid fixed-length unicode issues.\n",
    "                string_list = [str(x) for x in col_data]\n",
    "                grp.create_dataset(\n",
    "                    col, \n",
    "                    data=np.array(string_list, dtype=object), \n",
    "                    dtype=variable_length_string_dt, \n",
    "                    compression=\"gzip\"\n",
    "                )\n",
    "            else:\n",
    "                # For numeric or other dtypes, write directly\n",
    "                grp.create_dataset(col, data=col_data, compression=\"gzip\")\n",
    "        \n",
    "        # (B) Save relevant data_obj attributes\n",
    "        if hasattr(data_obj, \"embed_vals\") and data_obj.embed_vals is not None:\n",
    "            hf.create_dataset(\"embed_vals\", data=data_obj.embed_vals, compression=\"gzip\")\n",
    "        \n",
    "        # Example: saving meta info\n",
    "        if hasattr(data_obj, \"meta\") and data_obj.meta is not None:\n",
    "            meta_grp = hf.create_group(\"meta\")\n",
    "            if isinstance(data_obj.meta, dict):\n",
    "                for key, val in data_obj.meta.items():\n",
    "                    val_array = np.array(val, dtype=object)\n",
    "                    if val_array.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "                        variable_length_string_dt = h5py.special_dtype(vlen=str)\n",
    "                        meta_grp.create_dataset(\n",
    "                            key,\n",
    "                            data=np.array([str(x) for x in val_array.flatten()], dtype=object),\n",
    "                            dtype=variable_length_string_dt,\n",
    "                            compression=\"gzip\"\n",
    "                        )\n",
    "                    else:\n",
    "                        meta_grp.create_dataset(key, data=val_array, compression=\"gzip\")\n",
    "            else:\n",
    "                meta_vals = np.array(data_obj.meta, dtype=object)\n",
    "                if meta_vals.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "                    variable_length_string_dt = h5py.special_dtype(vlen=str)\n",
    "                    meta_vals = np.array([str(x) for x in meta_vals.flatten()], dtype=object)\n",
    "                    meta_grp.create_dataset(\n",
    "                        \"meta_data\",\n",
    "                        data=meta_vals,\n",
    "                        dtype=variable_length_string_dt,\n",
    "                        compression=\"gzip\"\n",
    "                    )\n",
    "                else:\n",
    "                    meta_grp.create_dataset(\"meta_data\", data=meta_vals, compression=\"gzip\")\n",
    "        \n",
    "        # (C) Save frame mapping info\n",
    "        map_grp = hf.create_group(\"frame_mapping\")\n",
    "        map_grp.create_dataset(\n",
    "            \"mapped_sixcam_frame_indices\",\n",
    "            data=np.array(map_data[\"mapped_sixcam_frame_indices\"]),\n",
    "            compression=\"gzip\"\n",
    "        )\n",
    "        map_grp.attrs[\"time_offset\"] = time_offset\n",
    "\n",
    "    print(f\"Filtered data for '{rec_path}' saved to '{out_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No conversion path for dtype: dtype('<U13')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m out_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/lq53/mir_repos/dappy_24_nov/byws_version/250116_wav_ffix_ang_pos/50_p/250325_w_dappy.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# f\"/home/lq53/mir_repos/dappy_24_nov/byws_version/250109_opti/{trrrry}/aligned_mir_walalala_filtered_data.h5\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mexport_aligned_data_to_h5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloaded_data_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrec_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrec_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_mapping_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_mapping_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_file\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 87\u001b[0m, in \u001b[0;36mexport_aligned_data_to_h5\u001b[0;34m(data_obj, rec_path, frame_mapping_file, out_file)\u001b[0m\n\u001b[1;32m     85\u001b[0m     variable_length_string_dt \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mspecial_dtype(vlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Convert to str without flattening the inner structure (each cell remains one string)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mgrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_length_string_dt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# For numeric or other dtypes, write directly\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     grp\u001b[38;5;241m.\u001b[39mcreate_dataset(col, data\u001b[38;5;241m=\u001b[39mcol_data, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/neuroposelib/lib/python3.9/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/miniconda3/envs/neuroposelib/lib/python3.9/site-packages/h5py/_hl/dataset.py:168\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001b[0m\n\u001b[1;32m    165\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m h5d\u001b[38;5;241m.\u001b[39mcreate(parent\u001b[38;5;241m.\u001b[39mid, name, tid, sid, dcpl\u001b[38;5;241m=\u001b[39mdcpl, dapl\u001b[38;5;241m=\u001b[39mdapl)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m--> 168\u001b[0m     \u001b[43mdset_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:271\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1693\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1759\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: No conversion path for dtype: dtype('<U13')"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# trrrry = '60_p'\n",
    "\n",
    "dts_p = \"/home/lq53/mir_repos/dappy_24_nov/byws_version/250116_wav_ffix_ang_pos/50_p/datastruct.p\"\n",
    "# f\"/home/lq53/mir_repos/dappy_24_nov/byws_version/250109_opti/{trrrry}/datastruct.p\"\n",
    "# Load the data structure\n",
    "with open(dts_p, \"rb\") as f:\n",
    "    loaded_data_obj = pickle.load(f)\n",
    "\n",
    "rec_path = \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_25/20241002PMCr2_17_05\"\n",
    "frame_mapping_file = \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_25/20241002PMCr2_17_05/MIR_Aligned/frame_mapping.json\"\n",
    "out_file = \"/home/lq53/mir_repos/dappy_24_nov/byws_version/250116_wav_ffix_ang_pos/50_p/250325_w_dappy.h5\"\n",
    "# f\"/home/lq53/mir_repos/dappy_24_nov/byws_version/250109_opti/{trrrry}/aligned_mir_walalala_filtered_data.h5\"\n",
    "\n",
    "export_aligned_data_to_h5(\n",
    "    data_obj=loaded_data_obj, \n",
    "    rec_path=rec_path, \n",
    "    frame_mapping_file=frame_mapping_file, \n",
    "    out_file=out_file\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroposelib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
