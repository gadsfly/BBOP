{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dappy env\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def export_aligned_data_to_h5(\n",
    "    data_obj, \n",
    "    rec_path, \n",
    "    frame_mapping_file, \n",
    "    out_file\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters data_obj to a specific rec_path and frames from frame_mapping_file,\n",
    "    then saves minimal data to an HDF5 file (with string columns stored\n",
    "    as variable-length UTF-8 strings).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Filter by Prediction_path\n",
    "    rec_mat_path = os.path.join(rec_path, \"DANNCE/predict00/save_data_AVG.mat\")\n",
    "    path_data = data_obj.data[data_obj.data[\"Prediction_path\"] == rec_mat_path].copy()\n",
    "    if path_data.empty:\n",
    "        print(f\"No data found for rec_path: {rec_mat_path}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Read the frame mapping JSON\n",
    "    with open(frame_mapping_file, \"r\") as f:\n",
    "        map_data = json.load(f)\n",
    "    mapped_frames = set(map_data[\"mapped_sixcam_frame_indices\"])\n",
    "    time_offset = map_data[\"time_offset\"]\n",
    "    \n",
    "    # 3. Filter rows by mapped frame indices (assuming your DataFrame has 'frame' column)\n",
    "    if \"frame\" not in path_data.columns:\n",
    "        raise ValueError(\"DataFrame does not have 'frame' column to filter by.\")\n",
    "    \n",
    "    # Adjust frames so that they start at 0\n",
    "    min_frame = path_data[\"frame\"].min()\n",
    "    path_data[\"frame\"] = path_data[\"frame\"] - min_frame\n",
    "\n",
    "    # 1. Create helper offsets DataFrame\n",
    "    offsets = pd.DataFrame({'offset': range(10)})\n",
    "\n",
    "    # 2. Cross-merge (pandas 1.2+ supports `how=\"cross\"`)\n",
    "    expanded = path_data.merge(offsets, how='cross')\n",
    "\n",
    "    # 3. Update the frame by adding the offset\n",
    "    expanded['frame'] = expanded['frame'] + expanded['offset']\n",
    "    expanded.drop(columns='offset', inplace=True)\n",
    "    \n",
    "    filtered_data = expanded[expanded[\"frame\"].isin(mapped_frames)]\n",
    "    \n",
    "    if filtered_data.empty:\n",
    "        print(\"No overlapping frames found between path_data and mapped_sixcam_frame_indices.\")\n",
    "    \n",
    "    # 4. Save to HDF5\n",
    "    os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "    # Use variable-length string dtype for columns that are (or become) text\n",
    "    variable_length_string_dt = h5py.special_dtype(vlen=str)\n",
    "    \n",
    "    with h5py.File(out_file, \"w\") as hf:\n",
    "        \n",
    "        #\n",
    "        # (A) Save filtered DataFrame columns\n",
    "        #\n",
    "        grp = hf.create_group(\"filtered_data\")\n",
    "        \n",
    "        for col in filtered_data.columns:\n",
    "            col_data = filtered_data[col].to_numpy()  # get as NumPy array\n",
    "            \n",
    "            # Check if it's string-like (object, unicode, or bytes)\n",
    "            if col_data.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "                # Convert each element to a Python string, store as variable-length UTF-8\n",
    "                # Flatten, map to str, then reshape to original shape if multi-dimensional\n",
    "                original_shape = col_data.shape\n",
    "                col_data = col_data.reshape(-1)  # flatten\n",
    "                col_data = np.array([str(item) for item in col_data], dtype=object)\n",
    "                col_data = col_data.reshape(original_shape)\n",
    "                \n",
    "                grp.create_dataset(\n",
    "                    col, \n",
    "                    data=col_data, \n",
    "                    dtype=variable_length_string_dt, \n",
    "                    compression=\"gzip\"\n",
    "                )\n",
    "            else:\n",
    "                # Numeric or other supported dtype can be written directly\n",
    "                grp.create_dataset(col, data=col_data, compression=\"gzip\")\n",
    "        \n",
    "        #\n",
    "        # (B) Save relevant data_obj attributes\n",
    "        #\n",
    "        if hasattr(data_obj, \"embed_vals\") and data_obj.embed_vals is not None:\n",
    "            hf.create_dataset(\"embed_vals\", data=data_obj.embed_vals, compression=\"gzip\")\n",
    "        \n",
    "        # Example: saving meta info\n",
    "        if hasattr(data_obj, \"meta\") and data_obj.meta is not None:\n",
    "            meta_grp = hf.create_group(\"meta\")\n",
    "            if isinstance(data_obj.meta, dict):\n",
    "                for key, val in data_obj.meta.items():\n",
    "                    # Convert to array for consistency\n",
    "                    val_array = np.array(val, dtype=object)  # object to handle strings\n",
    "                    # If it has any string/unicode, cast them properly\n",
    "                    if val_array.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "                        val_array = val_array.reshape(-1)\n",
    "                        val_array = np.array([str(item) for item in val_array], dtype=object)\n",
    "                        # Reshape back if needed (only if it's consistent)\n",
    "                        # But typically meta might be a 1D list, so might not need reshape\n",
    "                        \n",
    "                        meta_grp.create_dataset(\n",
    "                            key,\n",
    "                            data=val_array,\n",
    "                            dtype=variable_length_string_dt,\n",
    "                            compression=\"gzip\"\n",
    "                        )\n",
    "                    else:\n",
    "                        meta_grp.create_dataset(key, data=val_array, compression=\"gzip\")\n",
    "            else:\n",
    "                # Non-dict meta structure\n",
    "                meta_vals = np.array(data_obj.meta, dtype=object)\n",
    "                if meta_vals.dtype.kind in [\"O\", \"U\", \"S\"]:\n",
    "                    meta_vals = [str(item) for item in meta_vals.flatten()]\n",
    "                    meta_vals = np.array(meta_vals, dtype=object)\n",
    "                    meta_grp.create_dataset(\n",
    "                        \"meta_data\",\n",
    "                        data=meta_vals,\n",
    "                        dtype=variable_length_string_dt,\n",
    "                        compression=\"gzip\"\n",
    "                    )\n",
    "                else:\n",
    "                    meta_grp.create_dataset(\"meta_data\", data=meta_vals, compression=\"gzip\")\n",
    "        \n",
    "        #\n",
    "        # (C) Save frame mapping info\n",
    "        #\n",
    "        map_grp = hf.create_group(\"frame_mapping\")\n",
    "        map_grp.create_dataset(\n",
    "            \"mapped_sixcam_frame_indices\",\n",
    "            data=np.array(map_data[\"mapped_sixcam_frame_indices\"]),\n",
    "            compression=\"gzip\"\n",
    "        )\n",
    "        map_grp.attrs[\"time_offset\"] = time_offset\n",
    "\n",
    "    print(f\"Filtered data for '{rec_path}' saved to '{out_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data for '/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_25/20241002PMCr2_17_05' saved to '/home/lq53/mir_repos/dappy_24_nov/byws_version/250116_wav_ffix_ang_pos/50_p/aligned_mir_walalala_filtered_data.h5'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "trrrry = '60_p'\n",
    "\n",
    "dts_p = \"/home/lq53/mir_repos/dappy_24_nov/byws_version/250116_wav_ffix_ang_pos/50_p/datastruct.p\"\n",
    "# f\"/home/lq53/mir_repos/dappy_24_nov/byws_version/250109_opti/{trrrry}/datastruct.p\"\n",
    "# Load the data structure\n",
    "with open(dts_p, \"rb\") as f:\n",
    "    loaded_data_obj = pickle.load(f)\n",
    "\n",
    "rec_path = \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_25/20241002PMCr2_17_05\"\n",
    "frame_mapping_file = \"/data/big_rim/rsync_dcc_sum/Oct3V1/2024_10_25/20241002PMCr2_17_05/MIR_Aligned/frame_mapping.json\"\n",
    "out_file = \"/home/lq53/mir_repos/dappy_24_nov/byws_version/250116_wav_ffix_ang_pos/50_p/aligned_mir_walalala_filtered_data.h5\"\n",
    "# f\"/home/lq53/mir_repos/dappy_24_nov/byws_version/250109_opti/{trrrry}/aligned_mir_walalala_filtered_data.h5\"\n",
    "\n",
    "export_aligned_data_to_h5(\n",
    "    data_obj=loaded_data_obj, \n",
    "    rec_path=rec_path, \n",
    "    frame_mapping_file=frame_mapping_file, \n",
    "    out_file=out_file\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroposelib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
