{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log for 2social_mini_0605pmc_single_15_38 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_30/2social_mini_0605pmc_single_15_38/folder_log.parquet\n",
      "Log for 2social_mini_0605pmc_single_15_18 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_30/2social_mini_0605pmc_single_15_18/folder_log.parquet\n",
      "Log for 2social_mini_0605pmc_single_15_00 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_30/2social_mini_0605pmc_single_15_00/folder_log.parquet\n",
      "Log for 24Anshu_f_paint_2mice saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_03_micecolor_test/24Anshu_f_paint_2mice/folder_log.parquet\n",
      "Log for 24Anshu_f_bleach_2mice saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_03_micecolor_test/24Anshu_f_bleach_2mice/folder_log.parquet\n",
      "Log for 24Anshu_f_paint_2mice_2 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_03_micecolor_test/24Anshu_f_paint_2mice_2/folder_log.parquet\n",
      "Log for 2social_mini_20240819V1r1_femalebleach_11_48 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_31/2social_mini_20240819V1r1_femalebleach_11_48/folder_log.parquet\n",
      "Log for 2social_mini_20240819V1r1_single_11_29 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_31/2social_mini_20240819V1r1_single_11_29/folder_log.parquet\n",
      "Log for 2social_mini_0605pmc_single_14_08 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_31/2social_mini_0605pmc_single_14_08/folder_log.parquet\n",
      "Log for 20240919v1l5r1mini_p20240717PMC_social_test_11_30 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r1mini_p20240717PMC_social_test_11_30/folder_log.parquet\n",
      "Log for 20240919v1l5r2mini_p20240717PMC_social_test_13_16 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r2mini_p20240717PMC_social_test_13_16/folder_log.parquet\n",
      "Log for 20240919v1l5r2mini_p20240521b1697576_social_test_13_26 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r2mini_p20240521b1697576_social_test_13_26/folder_log.parquet\n",
      "Log for 20240919v1l5r1mini_11_21 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21/folder_log.parquet\n",
      "Log for 20240919v1l5r2mini_13_09 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r2mini_13_09/folder_log.parquet\n",
      "Log for 20241002PMCLE1mini_p20241001RE2_14_10 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_27/20241002PMCLE1mini_p20241001RE2_14_10/folder_log.parquet\n",
      "Log for 20241002PMCLE1mini_13_59 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_27/20241002PMCLE1mini_13_59/folder_log.parquet\n",
      "Log for 20241015PMCBE1mini_p20241015PMCRE1_12_33 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_27/20241015PMCBE1mini_p20241015PMCRE1_12_33/folder_log.parquet\n",
      "Log for 20241015PMCBE1mini_12_24 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_27/20241015PMCBE1mini_12_24/folder_log.parquet\n",
      "Log for 20241001PMCRE2mini_13_57 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_12/20241001PMCRE2mini_13_57/folder_log.parquet\n",
      "Log for 20241001PMCRE2mini_15_22 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_12/20241001PMCRE2mini_15_22/folder_log.parquet\n",
      "Log for 20241001PMCRE2mini_15_35 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_12/20241001PMCRE2mini_15_35/folder_log.parquet\n",
      "Log for 20241001PMCRE2mini_13_44 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_12/20241001PMCRE2mini_13_44/folder_log.parquet\n",
      "Log for 20241225PMCLE1mini_11_57 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_13/20241225PMCLE1mini_11_57/folder_log.parquet\n",
      "Log for 20241225PMCLE1mini_11_37 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_13/20241225PMCLE1mini_11_37/folder_log.parquet\n",
      "Log for 20241225PMCLE1mini_11_23 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_13/20241225PMCLE1mini_11_23/folder_log.parquet\n",
      "Log for 20241225PMCLE1mini_11_06 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_13/20241225PMCLE1mini_11_06/folder_log.parquet\n",
      "Log for 20240819V1r1_13_41 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_17/20240819V1r1_13_41/folder_log.parquet\n",
      "Log for 20240819V1r1_14_40 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_17/20240819V1r1_14_40/folder_log.parquet\n",
      "Log for 20240819V1r1_14_25 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_17/20240819V1r1_14_25/folder_log.parquet\n",
      "Log for 20241002PMCr1_left1_ saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_24/20241002PMCr1_left1_/folder_log.parquet\n",
      "Log for 20241001PMCr2_15_07 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_24/20241001PMCr2_15_07/folder_log.parquet\n",
      "Log for 202420717PMCr1_ saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_24/202420717PMCr1_/folder_log.parquet\n",
      "Log for 20241001PMCr2_16_19 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_24/20241001PMCr2_16_19/folder_log.parquet\n",
      "Log for 20240919v1l5r1mini_p20240820v1r2_social_test_14_55 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_11/20240919v1l5r1mini_p20240820v1r2_social_test_14_55/folder_log.parquet\n",
      "Log for 20240919v1l5r1mini_pfemale_social_test_15_35 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_11/20240919v1l5r1mini_pfemale_social_test_15_35/folder_log.parquet\n",
      "Log for 20241015pmcr2_AO_12_52 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_07/20241015pmcr2_AO_12_52/folder_log.parquet\n",
      "Log for 2social_mini_20241015pmcr2_single_AO_13_24 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_07/2social_mini_20241015pmcr2_single_AO_13_24/folder_log.parquet\n",
      "Log for 20241015pmcr1_AO_14_52 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_07/20241015pmcr1_AO_14_52/folder_log.parquet\n",
      "Log for 20240916v1r1_15_05_30min saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_07/20240916v1r1_15_05_30min/folder_log.parquet\n",
      "Log for 20240916v1r1_19_18 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_07/20240916v1r1_19_18/folder_log.parquet\n",
      "Log for 20240916v1r1_17_55 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_07/20240916v1r1_17_55/folder_log.parquet\n",
      "Log for 2social_mini_0605pmc_single_18_25 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_29/2social_mini_0605pmc_single_18_25/folder_log.parquet\n",
      "Log for 2social_mini_0605pmc_single_18_42 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_29/2social_mini_0605pmc_single_18_42/folder_log.parquet\n",
      "Log for 2social_mini_0605pmc_single_18_00 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_29/2social_mini_0605pmc_single_18_00/folder_log.parquet\n",
      "Log for 20240819V1r2 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_03/20240819V1r2/folder_log.parquet\n",
      "Log for 20240916V1r2 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_04/20240916V1r2/folder_log.parquet\n",
      "Log for 20240819V1r1_20_10 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_04/20240819V1r1_20_10/folder_log.parquet\n",
      "Log for 20240819V1r1_21_05 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_04/20240819V1r1_21_05/folder_log.parquet\n",
      "Log for 20240916V1r1 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_04/20240916V1r1/folder_log.parquet\n",
      "Log for 20240819V1r1_21_40 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_04/20240819V1r1_21_40/folder_log.parquet\n",
      "Log for 20240819V1r1 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_04/20240819V1r1/folder_log.parquet\n",
      "Log for 20240910v1r_cricket_cyliner_test_16_17 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_13/20240910v1r_cricket_cyliner_test_16_17/folder_log.parquet\n",
      "Log for 240605pmc_righthole_acrylic_test_14_55 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_13/240605pmc_righthole_acrylic_test_14_55/folder_log.parquet\n",
      "Log for 240605pmc_righthole_cricket_acrylic_test_15_05 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_13/240605pmc_righthole_cricket_acrylic_test_15_05/folder_log.parquet\n",
      "Log for 20240916v1r1_16_53 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_14/20240916v1r1_16_53/folder_log.parquet\n",
      "Log for 20240916v1r1_14_53 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_14/20240916v1r1_14_53/folder_log.parquet\n",
      "Log for 20240916v1r1_16_35 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_14/20240916v1r1_16_35/folder_log.parquet\n",
      "Log for 20240916v1r2_15_58 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_14/20240916v1r2_15_58/folder_log.parquet\n",
      "Log for 20240916v1r2_14_30 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_14/20240916v1r2_14_30/folder_log.parquet\n",
      "Log for 20240916v1r1_16_37 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_14/20240916v1r1_16_37/folder_log.parquet\n",
      "Log for 20240707-PMC-r1 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_09_18/20240707-PMC-r1/folder_log.parquet\n",
      "Log for 20240819_V1_r1_2 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_09_18/20240819_V1_r1_2/folder_log.parquet\n",
      "Log for 20240819_V1_r1 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_09_18/20240819_V1_r1/folder_log.parquet\n",
      "Log for 20240819-V1-r2 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_09_18/20240819-V1-r2/folder_log.parquet\n",
      "Log for 0cricket_test_all_dead_3_14_50 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_06/0cricket_test_all_dead_3_14_50/folder_log.parquet\n",
      "Log for 0cricket_test_1 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_06/0cricket_test_1/folder_log.parquet\n",
      "Log for 0cricket_test_3_14_32 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_06/0cricket_test_3_14_32/folder_log.parquet\n",
      "Log for 0cricket_test_2_14_00 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_06/0cricket_test_2_14_00/folder_log.parquet\n",
      "Log for 20241015pmcr2_17_13 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_06/20241015pmcr2_17_13/folder_log.parquet\n",
      "Log for 20241015pmcr1_16_12 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_06/20241015pmcr1_16_12/folder_log.parquet\n",
      "Log for 20241015pmcr2_16_53 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_06/20241015pmcr2_16_53/folder_log.parquet\n",
      "Log for 20240919v1l5r2mini_13_54 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54/folder_log.parquet\n",
      "Log for 20240919v1l5r1mini_p20240717PMC_social_14_31 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20240919v1l5r1mini_p20240717PMC_social_14_31/folder_log.parquet\n",
      "Log for 20240919v1l5r2mini_p20240717PMC_social_14_04 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20240919v1l5r2mini_p20240717PMC_social_14_04/folder_log.parquet\n",
      "Log for 20241212v1l1r2mini_p20241111_social_12_16 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r2mini_p20241111_social_12_16/folder_log.parquet\n",
      "Log for 20241212v1l1r1mini_11_22 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r1mini_11_22/folder_log.parquet\n",
      "Log for 20241212v1l1r2mini_12_04 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r2mini_12_04/folder_log.parquet\n",
      "Log for 20241212v1l1r2mini_11_56 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r2mini_11_56/folder_log.parquet\n",
      "Log for 2male_mice_test_miniscope saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/2male_mice_test_miniscope/folder_log.parquet\n",
      "Log for 20241002PMCr1_14_54 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/20241002PMCr1_14_54/folder_log.parquet\n",
      "Log for 20241002PMCr2_17_05 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/20241002PMCr2_17_05/folder_log.parquet\n",
      "Log for 2male_mice_test saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/2male_mice_test/folder_log.parquet\n",
      "Log for 20240916v1r2_14_30 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/20240916v1r2_14_30/folder_log.parquet\n",
      "Log for 20241002PMCr2_16_25 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/20241002PMCr2_16_25/folder_log.parquet\n",
      "Log for 20241002PMCr2_15_42 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/20241002PMCr2_15_42/folder_log.parquet\n",
      "Log for 20240819V1r1_AO_14_56 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_01/20240819V1r1_AO_14_56/folder_log.parquet\n",
      "Log for 2social_mini_20240910V1r_AO_single_12_50 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_01/2social_mini_20240910V1r_AO_single_12_50/folder_log.parquet\n",
      "Log for 20240910V1r_AO_12_50 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_01/20240910V1r_AO_12_50/folder_log.parquet\n",
      "Log for 20240910V1r_BO_11_35 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_01/20240910V1r_BO_11_35/folder_log.parquet\n",
      "Log for 2social_mini_20240819V1r1_AO_single_14_30 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_11_01/2social_mini_20240819V1r1_AO_single_14_30/folder_log.parquet\n",
      "Log for 20240819V1r1_20_00 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240819V1r1_20_00/folder_log.parquet\n",
      "Log for 20240916v1r1_15_35 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240916v1r1_15_35/folder_log.parquet\n",
      "Log for 20240916v1r1_14_40 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240916v1r1_14_40/folder_log.parquet\n",
      "Log for 20240916v1r1_16_03 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240916v1r1_16_03/folder_log.parquet\n",
      "Log for 20240916v1r1_14_20 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240916v1r1_14_20/folder_log.parquet\n",
      "Log for 20240819V1r1_19_44 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240819V1r1_19_44/folder_log.parquet\n",
      "Log for 20240819V1r1_20_17 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240819V1r1_20_17/folder_log.parquet\n",
      "Log for 20240819V1r1_20_32 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240819V1r1_20_32/folder_log.parquet\n",
      "Log for 20240916v1r1_16_22 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240916v1r1_16_22/folder_log.parquet\n",
      "Log for 20240916v1r1_15_53 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240916v1r1_15_53/folder_log.parquet\n",
      "Log for 20240819V1r1_20_48 saved at /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_08/20240819V1r1_20_48/folder_log.parquet\n"
     ]
    }
   ],
   "source": [
    "from utlis.sync_utlis.sync_df_utlis import find_calib_file\n",
    "from utlis.scan_engine_utlis.scan_engine_utlis import (\n",
    "    read_failed_paths,\n",
    "    match_date_pattern,\n",
    "    assign_status_codes,\n",
    ")\n",
    "# from scan_engine.status_fields_config_24summ import STATUS_FIELDS_CONFIG\n",
    "from status_fields_config_oct3v1_ssh_updated20250305 import STATUS_FIELDS_CONFIG\n",
    "# Import functions from utils.py\n",
    "from utlis.scan_engine_utlis.scan_log_utlis import (\n",
    "    load_scan_log,\n",
    "    save_scan_log,\n",
    "    clean_scan_log,\n",
    "    update_scan_log,\n",
    "    get_folders_to_scan\n",
    ")\n",
    "\n",
    "def scan_folder(folder_name, base_folder, failed_paths, config, rec_files_to_scan):\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    rec_files_data = []  # To store rec files and their status\n",
    "    calib_files = []  # To store calibration files\n",
    "\n",
    "    # Check for calibration files starting with 'calib'\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(\"calib\"):\n",
    "            calib_files.append(file_name)\n",
    "\n",
    "    # Traverse subfolders within this folder\n",
    "    for subfolder_name in rec_files_to_scan:\n",
    "        subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "\n",
    "        # Check for subfolders starting with a digit (rec folders)\n",
    "        if os.path.isdir(subfolder_path) and subfolder_name[0].isdigit():\n",
    "            # Find calibration file for each subfolder\n",
    "            calib_file = find_calib_file(subfolder_path)\n",
    "\n",
    "            # Assign status codes dynamically based on the config\n",
    "            rec_file_data = assign_status_codes(\n",
    "                folder_name, subfolder_path, calib_file, failed_paths, config\n",
    "            )\n",
    "\n",
    "            rec_file_data['rec_file'] = subfolder_name  # Add rec_file to the data\n",
    "            # Add date-time for update and some future\n",
    "            rec_file_data['scan_time'] = datetime.datetime.now().isoformat()\n",
    "\n",
    "            rec_files_data.append(rec_file_data)\n",
    "\n",
    "    return {\n",
    "        'date_folder': folder_name,\n",
    "        'calib_files': calib_files,  # Store the calibration files under date_folder level\n",
    "        'rec_files_data': rec_files_data  # Each rec file with its status fields\n",
    "    }\n",
    "\n",
    "def log_folder_to_parquet_sep(base_folder, failed_paths_file, config, force_rescan_rec_files=None, rescan_threshold_days=7):\n",
    "    \"\"\"Log folders and save Parquet in subfolders with partial scan support.\"\"\"\n",
    "\n",
    "    # Paths for scan log\n",
    "    scan_log_path = os.path.join(base_folder, 'paret', 'scan_log.csv')\n",
    "\n",
    "    # Load or initialize the scan log\n",
    "    scan_log_df = load_scan_log(scan_log_path)\n",
    "\n",
    "    # Read manually inputted failed paths\n",
    "    failed_paths = read_failed_paths(failed_paths_file) if failed_paths_file else set()\n",
    "\n",
    "    # Forced rescans\n",
    "    # force_rescan_rec_files = [\n",
    "    #     # ('2023-10-01', '001'),\n",
    "    #     # ('2023-10-02', '002'),\n",
    "    #     # Add more as needed\n",
    "    # ]\n",
    "    # force_rescan_rec_files_set = set(force_rescan_rec_files)\n",
    "    \n",
    "    if force_rescan_rec_files is None:\n",
    "        force_rescan_rec_files = []\n",
    "    force_rescan_rec_files_set = set(force_rescan_rec_files)\n",
    "\n",
    "\n",
    "\n",
    "    # Rescan threshold\n",
    "    # rescan_threshold_days = 7\n",
    "\n",
    "    # Determine folders to scan\n",
    "    folders_to_scan = get_folders_to_scan(base_folder, scan_log_df, rescan_threshold_days, force_rescan_rec_files_set)\n",
    "\n",
    "    if not folders_to_scan:\n",
    "        print(\"No new or modified folders to scan.\")\n",
    "        return\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel folder scanning\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for date_folder, rec_files_to_scan in folders_to_scan.items():\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    scan_folder,\n",
    "                    date_folder,\n",
    "                    base_folder,\n",
    "                    failed_paths,\n",
    "                    config,\n",
    "                    rec_files_to_scan\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            folder_log = future.result()\n",
    "            date_folder = folder_log['date_folder']\n",
    "            calib_files = folder_log.get('calib_files', [])\n",
    "\n",
    "            # Ensure 'calib_files' is always a list of strings\n",
    "            calib_files = [str(f) for f in calib_files] if calib_files else []\n",
    "\n",
    "            # Process and save each experiment's log separately\n",
    "            for rec_file_data in folder_log['rec_files_data']:\n",
    "                rec_file = rec_file_data['rec_file']\n",
    "                subfolder_save_path = os.path.join(base_folder, date_folder, rec_file, \"folder_log.parquet\")\n",
    "\n",
    "                # Ensure the experiment/rec_file folder exists\n",
    "                os.makedirs(os.path.dirname(subfolder_save_path), exist_ok=True)\n",
    "\n",
    "                # Add 'date_folder' and 'calib_files' to rec_file_data\n",
    "                rec_file_data['date_folder'] = date_folder\n",
    "                rec_file_data['calib_files'] = calib_files\n",
    "\n",
    "                # Dynamically ensure all relevant columns are strings based on config\n",
    "                status_columns = list(config.keys())\n",
    "                df = pd.DataFrame([rec_file_data])\n",
    "                df[status_columns] = df[status_columns].astype(str)\n",
    "\n",
    "                # Convert the data into a DataFrame and save the Parquet file\n",
    "                table = pa.Table.from_pandas(df)\n",
    "                pq.write_table(table, subfolder_save_path)\n",
    "\n",
    "                print(f\"Log for {rec_file} saved at {subfolder_save_path}\")\n",
    "\n",
    "                # Update the scan log\n",
    "                scan_log_df = update_scan_log(scan_log_df, date_folder, rec_file)\n",
    "\n",
    "    # Clean up the scan log\n",
    "    scan_log_df = clean_scan_log(scan_log_df, base_folder)\n",
    "\n",
    "    # Save the updated scan log\n",
    "    save_scan_log(scan_log_df, scan_log_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_folder = \"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1\" #\"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ\"  # Replace with your base folder\n",
    "    # save_path = os.path.join(base_folder, 'paret')\n",
    "    failed_paths_file = '/hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/sync_failed_brws.txt'  # File containing failed paths\n",
    "\n",
    "\n",
    "    force_rescan_rec_files = [\n",
    "        # ('2023-10-01', '001'),\n",
    "        # ('2023-10-02', '002'),\n",
    "        # Add more as needed\n",
    "    ]\n",
    "    rescan_threshold_days = 0.0001 # 7 days, but guess if i mess up i can just change it to automatically rescan all, smile... #0.1\n",
    "\n",
    "    log_folder_to_parquet_sep(base_folder, failed_paths_file, STATUS_FIELDS_CONFIG,\n",
    "                              force_rescan_rec_files=force_rescan_rec_files,\n",
    "                              rescan_threshold_days=rescan_threshold_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(os.path.abspath('../..'))\n",
    "from utlis.scan_engine_utlis.scan_engine_utlis import read_all_parquet_files\n",
    "# base_folder = \"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ\"\n",
    "all_df = read_all_parquet_files(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mir_generate_param sync dropf_handle com social miniscope test  \\\n",
      "0                  1    1            0   0      0         1    1   \n",
      "1                  1    1            0   0      0         1    1   \n",
      "2                  1    1            0   0      0         1    0   \n",
      "3                  1    1            0   0      0         1    0   \n",
      "4                  1    1            0   0      0         1    0   \n",
      "5                  1    1            0   0      0         1    0   \n",
      "6                  1    1            0   0      0         1    0   \n",
      "7                  1    1            0   0      0         1    0   \n",
      "\n",
      "  after_oxytocin before_oxytocin dannce mini_rec_sync  \\\n",
      "0              0               0      0             0   \n",
      "1              0               0      0             0   \n",
      "2              0               0      0             0   \n",
      "3              0               0      0             0   \n",
      "4              0               0      0             0   \n",
      "5              0               0      0             0   \n",
      "6              0               0      0             0   \n",
      "7              0               0      0             0   \n",
      "\n",
      "                    rec_file                   scan_time date_folder  \\\n",
      "0            2male_mice_test  2025-03-10T16:49:52.791218  2024_10_25   \n",
      "1  2male_mice_test_miniscope  2025-03-10T16:49:52.627879  2024_10_25   \n",
      "2   20240919v1l5r1mini_11_21  2025-03-10T16:49:52.321387  2024_12_18   \n",
      "3   20240919v1l5r2mini_13_09  2025-03-10T16:49:52.453731  2024_12_18   \n",
      "4   20240919v1l5r2mini_13_54  2025-03-10T16:49:52.656493  2024_12_31   \n",
      "5   20241212v1l1r1mini_11_22  2025-03-10T16:49:52.802510  2024_12_31   \n",
      "6   20241212v1l1r2mini_12_04  2025-03-10T16:49:52.828279  2024_12_31   \n",
      "7   20241001PMCRE2mini_13_57  2025-03-10T16:49:52.236870  2025_02_12   \n",
      "\n",
      "                                         calib_files  \n",
      "0  [calib_15_40, calib_14_51, calib_16_55, calib_...  \n",
      "1  [calib_15_40, calib_14_51, calib_16_55, calib_...  \n",
      "2                        [calib_before, calib_after]  \n",
      "3                        [calib_before, calib_after]  \n",
      "4                                     [calib_before]  \n",
      "5                                     [calib_before]  \n",
      "6                                     [calib_before]  \n",
      "7           [calib_14_11, calib_before, calib_after]  \n"
     ]
    }
   ],
   "source": [
    "import pyarrow.compute as pc\n",
    "table = all_df #combined_df\n",
    "# Filter mir_generate_param == 0 and sync != 3\n",
    "filter_mask = pc.and_(\n",
    "    # pc.equal(table['sync'], '1'),\n",
    "    # pc.not_equal(table['com'], '1')\n",
    "    pc.equal(table['sync'], '1'),\n",
    "    pc.and_(\n",
    "                pc.equal(table['com'], '0'),\n",
    "                pc.not_equal(table['social'], '1')\n",
    "            ),\n",
    ")\n",
    "\n",
    "# Apply the filter and print the results\n",
    "for_com = table.filter(filter_mask)\n",
    "\n",
    "# Print each row of the filtered table\n",
    "print(for_com.to_pandas())  # This will display the filtered data in a familiar pandas-like format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "mir_generate_param: string\n",
       "sync: string\n",
       "dropf_handle: string\n",
       "com: string\n",
       "rec_file: string\n",
       "scan_time: string\n",
       "date_folder: string\n",
       "calib_files: list<element: string>\n",
       "  child 0, element: string\n",
       "----\n",
       "mir_generate_param: [[\"1\"],[\"1\"],...,[\"1\"],[\"1\"]]\n",
       "sync: [[\"1\"],[\"1\"],...,[\"1\"],[\"1\"]]\n",
       "dropf_handle: [[\"0\"],[\"0\"],...,[\"0\"],[\"0\"]]\n",
       "com: [[\"0\"],[\"0\"],...,[\"0\"],[\"0\"]]\n",
       "rec_file: [[\"1686940_no_holes\"],[\"1686940_left_right_2\"],...,[\"24Anshu_f_paint_2mice\"],[\"24Anshu_f_paint_2mice_2\"]]\n",
       "scan_time: [[\"2024-11-05T14:07:26.137916\"],[\"2024-11-05T14:07:26.127355\"],...,[\"2024-11-05T14:07:26.068943\"],[\"2024-11-05T14:07:26.119025\"]]\n",
       "date_folder: [[\"2024_06_26\"],[\"2024_06_27\"],...,[\"2024_10_03_micecolor_test\"],[\"2024_10_03_micecolor_test\"]]\n",
       "calib_files: [[[\"calib_before\",\"calib_stop2\",\"calib_stop3\",\"calib_before2\"]],[[\"calib_after_6\",\"calib_after_3\",\"calib_after_7\",\"calib_before3\",\"calib_before_6\",\"calib_before_4\",\"calib_before_7\",\"calib_after_4\",\"calib_after_5\",\"calib_before_5\"]],...,[[\"calib_19_55\",\"calib_before\",\"calib_15_25\"]],[[\"calib_19_55\",\"calib_before\",\"calib_15_25\"]]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Function to create the full expdir path and run the command\n",
    "# def run_command(base_path, date_folder, rec_file):\n",
    "#     expdir_path = os.path.join(base_path, date_folder, rec_file)\n",
    "#     command = f\"python slurm_launch_predict.py --expdir {expdir_path} --predict_com\"\n",
    "#     print(f\"Executing command: {command}\")\n",
    "#     os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r2mini_12_04 --predict_comExecuting command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/2male_mice_test --predict_com\n",
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/2male_mice_test_miniscope --predict_com\n",
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_12/20241001PMCRE2mini_13_57 --predict_com\n",
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r1mini_11_22 --predict_com\n",
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r2mini_13_09 --predict_com\n",
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21 --predict_com\n",
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54 --predict_com\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/2male_mice_test_miniscope\n",
      "dannce predict com /hpc/group/tdunn/tqxli/sdannce_scripts/configs/com_mouse_config.yaml --com-predict-weights=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/COM/checkpoint-epoch20.pth --com-predict-dir=COM/predict00 --max-num-samples 100000 --batch-size=1\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/2male_mice_test_miniscope/slurm/predict_com_2male_mice_test_miniscope.out\n",
      "Submitted batch job 26718174\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/2male_mice_test\n",
      "dannce predict com /hpc/group/tdunn/tqxli/sdannce_scripts/configs/com_mouse_config.yaml --com-predict-weights=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/COM/checkpoint-epoch20.pth --com-predict-dir=COM/predict00 --max-num-samples 100000 --batch-size=1\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_10_25/2male_mice_test/slurm/predict_com_2male_mice_test.out\n",
      "Submitted batch job 26718175\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_12/20241001PMCRE2mini_13_57\n",
      "dannce predict com /hpc/group/tdunn/tqxli/sdannce_scripts/configs/com_mouse_config.yaml --com-predict-weights=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/COM/checkpoint-epoch20.pth --com-predict-dir=COM/predict00 --max-num-samples 100000 --batch-size=1\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2025_02_12/20241001PMCRE2mini_13_57/slurm/predict_com_20241001PMCRE2mini_13_57.out\n",
      "Submitted batch job 26718176\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21\n",
      "dannce predict com /hpc/group/tdunn/tqxli/sdannce_scripts/configs/com_mouse_config.yaml --com-predict-weights=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/COM/checkpoint-epoch20.pth --com-predict-dir=COM/predict00 --max-num-samples 100000 --batch-size=1\n",
      "Copying default io file /hpc/group/tdunn/tqxli/sdannce_scripts/io_files/io.yaml to /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r1mini_11_21/slurm/predict_com_20240919v1l5r1mini_11_21.out\n",
      "Submitted batch job 26718177\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54\n",
      "dannce predict com /hpc/group/tdunn/tqxli/sdannce_scripts/configs/com_mouse_config.yaml --com-predict-weights=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/COM/checkpoint-epoch20.pth --com-predict-dir=COM/predict00 --max-num-samples 100000 --batch-size=1\n",
      "Copying default io file /hpc/group/tdunn/tqxli/sdannce_scripts/io_files/io.yaml to /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20240919v1l5r2mini_13_54/slurm/predict_com_20240919v1l5r2mini_13_54.out\n",
      "Submitted batch job 26718178\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r2mini_13_09\n",
      "dannce predict com /hpc/group/tdunn/tqxli/sdannce_scripts/configs/com_mouse_config.yaml --com-predict-weights=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/COM/checkpoint-epoch20.pth --com-predict-dir=COM/predict00 --max-num-samples 100000 --batch-size=1\n",
      "Copying default io file /hpc/group/tdunn/tqxli/sdannce_scripts/io_files/io.yaml to /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r2mini_13_09\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_18/20240919v1l5r2mini_13_09/slurm/predict_com_20240919v1l5r2mini_13_09.out\n",
      "Submitted batch job 26718179\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r2mini_12_04\n",
      "dannce predict com /hpc/group/tdunn/tqxli/sdannce_scripts/configs/com_mouse_config.yaml --com-predict-weights=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/COM/checkpoint-epoch20.pth --com-predict-dir=COM/predict00 --max-num-samples 100000 --batch-size=1\n",
      "Copying default io file /hpc/group/tdunn/tqxli/sdannce_scripts/io_files/io.yaml to /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r2mini_12_04\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r2mini_12_04/slurm/predict_com_20241212v1l1r2mini_12_04.out\n",
      "Submitted batch job 26718180\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r1mini_11_22\n",
      "dannce predict com /hpc/group/tdunn/tqxli/sdannce_scripts/configs/com_mouse_config.yaml --com-predict-weights=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/COM/checkpoint-epoch20.pth --com-predict-dir=COM/predict00 --max-num-samples 100000 --batch-size=1\n",
      "Copying default io file /hpc/group/tdunn/tqxli/sdannce_scripts/io_files/io.yaml to /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r1mini_11_22\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/Oct3V1/2024_12_31/20241212v1l1r1mini_11_22/slurm/predict_com_20241212v1l1r1mini_11_22.out\n",
      "Submitted batch job 26718181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "\n",
    "# for_com = filtered_table\n",
    "slurm_launch_file = '/hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py'\n",
    "\n",
    "def check_expdir(expdir):\n",
    "    if not os.path.exists(expdir):\n",
    "        print(f\"Skipping: Experiment directory {expdir} does not exist\")\n",
    "        return None  # Return None or skip processing this directory\n",
    "    return expdir\n",
    "\n",
    "\n",
    "# Function to create the full expdir path and print the command for dry-run\n",
    "def run_command(base_path, date_folder, rec_file, dry_run=True):\n",
    "    expdir_path = os.path.join(base_path, date_folder, rec_file)\n",
    "    \n",
    "    # Check if the experiment directory exists before running the command\n",
    "    if check_expdir(expdir_path) is None:\n",
    "        return  # Skip this execution if the directory does not exist\n",
    "    \n",
    "    # command = f\"python {slurm_launch_file} --expdir {expdir_path} --predict_com\"\n",
    "    command = f\"conda run -n sdannce python {slurm_launch_file} --expdir {expdir_path} --predict_com\" # --allow_overwrite remove allow_overwrite by default.\n",
    "\n",
    "    if dry_run:\n",
    "        print(f\"[DRY-RUN] Command: {command}\")\n",
    "    else:\n",
    "        print(f\"Executing command: {command}\")\n",
    "        os.system(command)\n",
    "\n",
    "\n",
    "# Extract relevant data from the filtered PyArrow Table `for_com`\n",
    "base_path = base_folder\n",
    "# base_path = \"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ\"  # Adjust this base path as needed\n",
    "records = [\n",
    "    {\n",
    "        'date_folder': date_folder.as_py(),  # Convert to string using as_py()\n",
    "        'rec_file': rec_file.as_py()         # Convert to string using as_py()\n",
    "    }\n",
    "    for date_folder, rec_file in zip(for_com['date_folder'], for_com['rec_file'])\n",
    "]\n",
    "\n",
    "# Run in parallel with dry-run enabled for testing\n",
    "max_concurrent_jobs = 4 # chosing how mnay gpu to take, let's say maybe 4??? or 5... we're not urgent so...\n",
    "\n",
    "dry_run = False #True  # Set to False to execute commands\n",
    "# parallelly submission of jobs is just stupid....\n",
    "with ThreadPoolExecutor() as executor: #\n",
    "    futures = [\n",
    "        executor.submit(run_command, base_path, record['date_folder'], record['rec_file'], dry_run)\n",
    "        for record in records\n",
    "    ]\n",
    "\n",
    "# print(\"Dry-run test complete. Review the printed commands.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mir_generate_param sync dropf_handle com social miniscope test  \\\n",
      "0                   1    1            0   1      0         1    0   \n",
      "1                   1    1            0   1      0         1    0   \n",
      "2                   1    1            0   1      0         1    0   \n",
      "3                   1    1            0   1      0         1    0   \n",
      "4                   1    1            1   1      0         1    0   \n",
      "..                ...  ...          ...  ..    ...       ...  ...   \n",
      "57                  1    1            0   1      0         1    0   \n",
      "58                  1    1            0   1      0         1    0   \n",
      "59                  1    1            0   1      0         1    0   \n",
      "60                  1    1            0   1      0         1    0   \n",
      "61                  1    1            0   1      0         1    0   \n",
      "\n",
      "   after_oxytocin before_oxytocin dannce mini_rec_sync  \\\n",
      "0               0               0      1             0   \n",
      "1               0               0      1             0   \n",
      "2               0               0      1             0   \n",
      "3               0               0      1             0   \n",
      "4               0               0      0             0   \n",
      "..            ...             ...    ...           ...   \n",
      "57              0               0      0             0   \n",
      "58              0               0      0             0   \n",
      "59              0               0      0             0   \n",
      "60              0               0      0             0   \n",
      "61              0               0      0             0   \n",
      "\n",
      "                    rec_file                   scan_time date_folder  \\\n",
      "0            20240707-PMC-r1  2025-03-10T16:49:52.642613  2024_09_18   \n",
      "1             20240819-V1-r2  2025-03-10T16:49:52.829324  2024_09_18   \n",
      "2             20240819_V1_r1  2025-03-10T16:49:52.779187  2024_09_18   \n",
      "3           20240819_V1_r1_2  2025-03-10T16:49:52.689351  2024_09_18   \n",
      "4               20240819V1r2  2025-03-10T16:49:52.774021  2024_10_03   \n",
      "..                       ...                         ...         ...   \n",
      "57  20241225PMCLE1mini_11_06  2025-03-10T16:49:52.565024  2025_02_13   \n",
      "58  20241225PMCLE1mini_11_23  2025-03-10T16:49:52.328629  2025_02_13   \n",
      "59  20241225PMCLE1mini_11_37  2025-03-10T16:49:52.288512  2025_02_13   \n",
      "60  20241225PMCLE1mini_11_57  2025-03-10T16:49:52.239061  2025_02_13   \n",
      "61  20241015PMCBE1mini_12_24  2025-03-10T16:49:52.526026  2025_02_27   \n",
      "\n",
      "                                          calib_files  \n",
      "0                  [calib_before, calib_before_13_17]  \n",
      "1                  [calib_before, calib_before_13_17]  \n",
      "2                  [calib_before, calib_before_13_17]  \n",
      "3                  [calib_before, calib_before_13_17]  \n",
      "4                                      [calib_for_v1]  \n",
      "..                                                ...  \n",
      "57  [calib_12_12, calib_11_37, calib_13_34, calib_...  \n",
      "58  [calib_12_12, calib_11_37, calib_13_34, calib_...  \n",
      "59  [calib_12_12, calib_11_37, calib_13_34, calib_...  \n",
      "60  [calib_12_12, calib_11_37, calib_13_34, calib_...  \n",
      "61  [calib_before, calib_14_22, calib_12_52, calib...  \n",
      "\n",
      "[62 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.compute as pc\n",
    "table = all_df  # combined_df\n",
    "\n",
    "# Filter dropf_handle == 1, com != 1, and social != 1\n",
    "# filter_mask = pc.and_(\n",
    "#     # pc.equal(table['dropf_handle'], '1'),\n",
    "#     pc.not_equal(table['com'], '1'),\n",
    "#     pc.not_equal(table['social'], '1')\n",
    "# )\n",
    "\n",
    "# filter_mask = pc.and_(\n",
    "#     # pc.and_(\n",
    "#         # pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "        \n",
    "#         # pc.and_(\n",
    "#         #     pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "#         #     # pc.and_(\n",
    "#         #     #     pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "#         #     #     pc.not_equal(table['date_folder'], '2024_09_18')\n",
    "#         #     # ),\n",
    "#         #     pc.not_equal(table['date_folder'], '2024_11_07')\n",
    "#         # ),\n",
    "#     pc.equal(table['com'], '1'),\n",
    "#     # ),\n",
    "#     pc.not_equal(table['social'], '1')\n",
    "#     # pc.and_(\n",
    "#     #     pc.not_equal(table['social'], '1'),\n",
    "#     #     pc.equal(table['sync'], '1')\n",
    "#     # )\n",
    "# )\n",
    "filter_mask = pc.and_(\n",
    "        pc.equal(table['com'], '1'),\n",
    "                pc.not_equal(table['social'], '1')\n",
    "    # pc.equal(table['sync'], '1'),\n",
    "    # pc.not_equal(table['com'], '1')\n",
    "    # pc.equal(table['sync'], '1'),\n",
    "    # pc.and_(\n",
    "    #             pc.equal(table['com'], '0'),\n",
    "    #             pc.not_equal(table['social'], '1')\n",
    "    #         ),\n",
    ")\n",
    "# Apply the filter and print the results\n",
    "# for_com_vis = table.filter(pc.equal(table['com'], '1')) #filter_mask\n",
    "for_com_vis = table.filter(filter_mask) #filter_mask\n",
    "# Print each row of the filtered table\n",
    "print(for_com_vis.to_pandas())  # Display the filtered data in a familiar pandas-like format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis.vis_valid_utlis.com_trag_updated import plot_com_all\n",
    "\n",
    "# Assume base_folder is defined elsewhere in your script\n",
    "# base_folder = '/path/to/your/base/folder'\n",
    "\n",
    "records = [\n",
    "    {\n",
    "        'date_folder': date_folder.as_py(),  # Convert to string using as_py()\n",
    "        'rec_file': rec_file.as_py()         # Convert to string using as_py()\n",
    "    }\n",
    "    for date_folder, rec_file in zip(for_com_vis['date_folder'], for_com_vis['rec_file'])\n",
    "]\n",
    "\n",
    "# Iterate through the records and process each one sequentially\n",
    "for record in records:\n",
    "    base_path = f\"{base_folder}/{record['date_folder']}/{record['rec_file']}\"\n",
    "    print(base_path)\n",
    "    try:\n",
    "        plot_com_all(base_path) #com_folder_name='COM/predict00', perform_jump_indices=True, perform_video_generation=False, perform_generate_com_video=False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {base_path}: {e}\")\n",
    "        # Skip to the next record if an error occurs\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mir_generate_param sync saline caffeine first second habituation com dannce  \\\n",
      "0                  1    1      0        0     0      0           0   1      0   \n",
      "1                  1    1      0        0     0      0           0   1      0   \n",
      "2                  1    1      0        1     0      0           0   1      0   \n",
      "3                  1    1      1        0     0      0           0   1      0   \n",
      "4                  1    1      1        0     0      0           0   1      0   \n",
      "\n",
      "  test                          rec_file                   scan_time  \\\n",
      "0    0              1686940_left_right_2  2024-11-15T11:49:29.253522   \n",
      "1    0                1686941_left_right  2024-11-15T11:49:29.249835   \n",
      "2    0        1691486_left_caffeine_1128  2024-11-15T11:49:29.260755   \n",
      "3    0       1691486_no_hole_saline_1029  2024-11-15T11:49:29.274006   \n",
      "4    1  1691486_no_hole_saline_1054_test  2024-11-15T11:49:29.255455   \n",
      "\n",
      "  date_folder                                        calib_files  \n",
      "0  2024_06_27  [calib_after_6, calib_after_3, calib_after_7, ...  \n",
      "1  2024_06_27  [calib_after_6, calib_after_3, calib_after_7, ...  \n",
      "2  2024_07_03  [calib_before_label3d_dannce.mat, calib_1532, ...  \n",
      "3  2024_07_08  [calib_after_no_hole_1006, calib_after_no_hole...  \n",
      "4  2024_07_08  [calib_after_no_hole_1006, calib_after_no_hole...  \n"
     ]
    }
   ],
   "source": [
    "import pyarrow.compute as pc\n",
    "table = all_df  # combined_df\n",
    "\n",
    "# Filter dropf_handle == 1, com != 1, and social != 1\n",
    "# filter_mask = pc.and_(\n",
    "#     # pc.equal(table['dropf_handle'], '1'),\n",
    "#     pc.equal(table['com'], '1'),\n",
    "#     pc.not_equal(table['dannce'], '1'),\n",
    "#     # pc.and_(\n",
    "#     #     pc.equal(table['sync'], '1'),\n",
    "#     #     pc.not_equal(table['dannce'], '1'),\n",
    "#     # )\n",
    "#     # pc.not_equal(table['social'], '1')\n",
    "# )\n",
    "\n",
    "filter_mask = pc.and_(\n",
    "\n",
    "        pc.equal(table['com'], '1'),\n",
    "                pc.not_equal(table['social'], '1')\n",
    "    # pc.equal(table['sync'], '1'),\n",
    "    # pc.not_equal(table['com'], '1')\n",
    "    # pc.equal(table['sync'], '1'),\n",
    "    # pc.and_(\n",
    "    #             pc.equal(table['com'], '1'),\n",
    "    #             pc.not_equal(table['social'], '1')\n",
    "    #         ),\n",
    ")\n",
    "\n",
    "# filter_mask = pc.and_(\n",
    "#     # pc.and_(\n",
    "#         # pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "        \n",
    "#         # pc.and_(\n",
    "#         #     pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "#         #     # pc.and_(\n",
    "#         #     #     pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "#         #     #     pc.not_equal(table['date_folder'], '2024_09_18')\n",
    "#         #     # ),\n",
    "#         #     pc.not_equal(table['date_folder'], '2024_11_07')\n",
    "#         # ),\n",
    "#     pc.equal(table['com'], '1'),\n",
    "#     # ),\n",
    "#     pc.not_equal(table['social'], '1')\n",
    "#     # pc.and_(\n",
    "#     #     pc.not_equal(table['social'], '1'),\n",
    "#     #     pc.equal(table['sync'], '1')\n",
    "#     # )\n",
    "# )\n",
    "\n",
    "# Apply the filter and print the results\n",
    "for_dannce = table.filter(filter_mask) #filter_mask\n",
    "\n",
    "# Print each row of the filtered table\n",
    "print(for_dannce.to_pandas())  # Display the filtered data in a familiar pandas-like format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: 2024_06_27/1686941_left_right is in the skip list\n",
      "Skipping: 2024_07_03/1691486_left_caffeine_1128 is in the skip list\n",
      "Skipping: 2024_07_08/1691486_no_hole_saline_1029 is in the skip list\n",
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ/2024_06_27/1686940_left_right_2 --predict_dannce\n",
      "Executing command: conda run -n sdannce python /hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py --expdir /hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ/2024_07_08/1691486_no_hole_saline_1054_test --predict_dannce\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ/2024_06_27/1686940_left_right_2\n",
      "dannce predict dannce /hpc/group/tdunn/tqxli/sdannce_scripts/configs/dannce_mouse_config.yaml --dannce-predict-model=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/DANNCE/train04_20241111_ft_mouse_demo/checkpoint-epoch60.pth --dannce-predict-dir=DANNCE/predict00 --com-file=COM/predict00/com3d0.mat --max-num-samples 100000 --batch-size=1\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ/2024_06_27/1686940_left_right_2/slurm/predict_dannce_1686940_left_right_2.out\n",
      "Submitted batch job 19581120\n",
      "\n",
      "Specified a single expdir; running predictions ...\n",
      "Running predictions on 1 experiments ...\n",
      "Running command:  cd /hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ/2024_07_08/1691486_no_hole_saline_1054_test\n",
      "dannce predict dannce /hpc/group/tdunn/tqxli/sdannce_scripts/configs/dannce_mouse_config.yaml --dannce-predict-model=/hpc/group/tdunn/tqxli/sdannce_scripts/weights/DANNCE/train04_20241111_ft_mouse_demo/checkpoint-epoch60.pth --dannce-predict-dir=DANNCE/predict00 --com-file=COM/predict00/com3d0.mat --max-num-samples 100000 --batch-size=1\n",
      "Slurm out: /hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ/2024_07_08/1691486_no_hole_saline_1054_test/slurm/predict_dannce_1691486_no_hole_saline_1054_test.out\n",
      "Submitted batch job 19581121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# below is to exclude the bad com dir mannually, by inserting things in a txt file...\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "\n",
    "slurm_launch_file = '/hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py'\n",
    "\n",
    "def check_expdir(expdir):\n",
    "    if not os.path.exists(expdir):\n",
    "        print(f\"Skipping: Experiment directory {expdir} does not exist\")\n",
    "        return None  # Return None or skip processing this directory\n",
    "    return expdir\n",
    "\n",
    "# Function to create the full expdir path and print the command for dry-run\n",
    "def run_command(base_path, date_folder, rec_file, dry_run=True):\n",
    "    expdir_path = os.path.join(base_path, date_folder, rec_file)\n",
    "    \n",
    "    # Check if the experiment directory exists before running the command\n",
    "    if check_expdir(expdir_path) is None:\n",
    "        return  # Skip this execution if the directory does not exist\n",
    "    \n",
    "    command = f\"conda run -n sdannce python {slurm_launch_file} --expdir {expdir_path} --predict_dannce\"  # --allow_overwrite removed by default.\n",
    "\n",
    "    if dry_run:\n",
    "        print(f\"[DRY-RUN] Command: {command}\")\n",
    "    else:\n",
    "        print(f\"Executing command: {command}\")\n",
    "        os.system(command)\n",
    "\n",
    "# Read the list of relative paths to skip from the .txt file\n",
    "txt_file = '/hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ/paret/com_1point_reruns.txt'  # Update this to the path of your .txt file\n",
    "rel_paths_to_skip = set()\n",
    "with open(txt_file, 'r') as f:\n",
    "    for line in f:\n",
    "        rel_path = line.strip()\n",
    "        if rel_path:\n",
    "            rel_paths_to_skip.add(rel_path)\n",
    "\n",
    "# print(rel_paths_to_skip)\n",
    "\n",
    "# Extract relevant data from the filtered PyArrow Table `for_com_vis`\n",
    "base_path = base_folder  # Ensure base_folder is defined\n",
    "# base_path = \"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/24summ\"  # Adjust this base path as needed\n",
    "\n",
    "records = [\n",
    "    {\n",
    "        'date_folder': date_folder.as_py(),  # Convert to string using as_py()\n",
    "        'rec_file': rec_file.as_py()         # Convert to string using as_py()\n",
    "    }\n",
    "    for date_folder, rec_file in zip(for_dannce['date_folder'], for_dannce['rec_file'])\n",
    "]\n",
    "\n",
    "# Run in parallel with dry-run enabled for testing\n",
    "max_concurrent_jobs = 4  # Choose how many jobs to run in parallel\n",
    "\n",
    "dry_run = False  # Set to False to execute commands\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_concurrent_jobs) as executor:\n",
    "    futures = []\n",
    "    futures = []\n",
    "    for record in records:\n",
    "        # Create the relative path (date_folder/rec_file)\n",
    "        rel_path = os.path.join(record['date_folder'], record['rec_file'])\n",
    "        expdir_path = os.path.join(base_path, rel_path)\n",
    "\n",
    "        if expdir_path in rel_paths_to_skip:\n",
    "            print(f\"Skipping: {rel_path} is in the skip list\")\n",
    "            continue  # Skip this record if it's in the skip list\n",
    "\n",
    "        # Submit the job to the executor\n",
    "        futures.append(\n",
    "            executor.submit(run_command, base_path, record['date_folder'], record['rec_file'], dry_run)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mir_generate_param sync saline caffeine first second habituation com  \\\n",
      "0                   1    1      0        0     0      0           0   1   \n",
      "1                   1    1      0        0     0      0           0   1   \n",
      "2                   1    1      0        0     0      0           0   1   \n",
      "3                   1    1      0        0     0      0           0   1   \n",
      "4                   1    1      0        0     0      0           0   1   \n",
      "..                ...  ...    ...      ...   ...    ...         ...  ..   \n",
      "56                  1    1      0        0     0      0           0   1   \n",
      "57                  1    1      0        0     0      0           0   1   \n",
      "58                  1    1      0        0     0      0           0   1   \n",
      "59                  1    1      0        0     0      0           0   1   \n",
      "60                  1    1      0        0     0      0           0   1   \n",
      "\n",
      "   dannce test               rec_file                   scan_time date_folder  \\\n",
      "0       1    0           1686940_left  2024-11-15T11:49:29.251449  2024_06_26   \n",
      "1       1    0          1686940_left2  2024-11-15T11:49:28.945464  2024_06_26   \n",
      "2       1    0       1686940_no_holes  2024-11-15T11:49:29.280020  2024_06_26   \n",
      "3       1    0           1686941_left  2024-11-15T11:49:29.258883  2024_06_27   \n",
      "4       1    0   1686941_left_right_2  2024-11-15T11:49:29.278816  2024_06_28   \n",
      "..    ...  ...                    ...                         ...         ...   \n",
      "56      1    0  20240628_PMC_r1_11_43  2024-11-15T11:49:28.930583  2024_08_08   \n",
      "57      1    0  20240702_PMC_r1_12_02  2024-11-15T11:49:29.249763  2024_08_08   \n",
      "58      1    0         20240717_PMCr1  2024-11-15T11:49:28.929938  2024_08_26   \n",
      "59      1    0         20240717_PMCr2  2024-11-15T11:49:29.180340  2024_08_26   \n",
      "60      1    0         20240717_PMCr2  2024-11-15T11:49:29.262799  2024_08_29   \n",
      "\n",
      "                                          calib_files  \n",
      "0   [calib_before, calib_stop2, calib_stop3, calib...  \n",
      "1   [calib_before, calib_stop2, calib_stop3, calib...  \n",
      "2   [calib_before, calib_stop2, calib_stop3, calib...  \n",
      "3   [calib_after_6, calib_after_3, calib_after_7, ...  \n",
      "4   [calib_before_9, calib_after_10, calib_after_9...  \n",
      "..                                                ...  \n",
      "56                        [calib_before, calib_after]  \n",
      "57                        [calib_before, calib_after]  \n",
      "58                                                 []  \n",
      "59                                                 []  \n",
      "60                               [calib_before_18_49]  \n",
      "\n",
      "[61 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.compute as pc\n",
    "table = all_df  # combined_df\n",
    "\n",
    "# Filter dropf_handle == 1, com != 1, and social != 1\n",
    "# filter_mask = pc.and_(\n",
    "#     # pc.equal(table['dropf_handle'], '1'),\n",
    "#     pc.equal(table['com'], '1'),\n",
    "#     pc.equal(table['dannce'], '1'),\n",
    "#     # pc.not_equal(table['social'], '1')\n",
    "# )\n",
    "\n",
    "# filter_mask = pc.and_(\n",
    "#     # pc.and_(\n",
    "#         # pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "        \n",
    "#         # pc.and_(\n",
    "#         #     pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "#         #     # pc.and_(\n",
    "#         #     #     pc.not_equal(table['date_folder'], '2024_11_06'),\n",
    "#         #     #     pc.not_equal(table['date_folder'], '2024_09_18')\n",
    "#         #     # ),\n",
    "#         #     pc.not_equal(table['date_folder'], '2024_11_07')\n",
    "#         # ),\n",
    "#     pc.equal(table['com'], '1'),\n",
    "#     # ),\n",
    "#     pc.not_equal(table['social'], '1')\n",
    "#     # pc.and_(\n",
    "#     #     pc.not_equal(table['social'], '1'),\n",
    "#     #     pc.equal(table['sync'], '1')\n",
    "#     # )\n",
    "# )\n",
    "\n",
    "# Apply the filter and print the results\n",
    "for_dannce_vis = table.filter(pc.equal(table['dannce'], '1')) #filter_mask\n",
    "\n",
    "# Print each row of the filtered table\n",
    "print(for_dannce_vis.to_pandas())  # Display the filtered data in a familiar pandas-like format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from useful_files.sophie_check_dannce_mir_modif import calculate_dis\n",
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy.io as sio\n",
    "\n",
    "\n",
    "# def dannce_valid(base_path):\n",
    "\n",
    "#     # params_file_end = 'big_label3d_dannce.mat' # camera parameters\n",
    "#     save_path = os.path.join(base_path, 'DANNCE/predict00', 'vis')\n",
    "#     pred_mat = 'save_data_AVG.mat'\n",
    "#     # Check if the savePath exists\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "#     temp_pred_mat = os.path.join(base_path, 'DANNCE/predict00',pred_mat)\n",
    "#     # ground truth according to true labels\n",
    "#     ground_truth_average = [19.18, 50.93, 10.59, 10.8]\n",
    "#     ground_truth_std = [2.9, 8, 2.36, 2.48]\n",
    "#     labels = ['BetweenEars', 'Trunk', 'LeftHind', 'RightHind']\n",
    "\n",
    "#     # Check if the prediction file exists\n",
    "#     if not os.path.exists(temp_pred_mat):\n",
    "#         print(f\"Prediction file '{pred_mat}' not found in '{base_path}'.\")\n",
    "#         return\n",
    "\n",
    "#     # Load prediction data\n",
    "#     try:\n",
    "#         pre = sio.loadmat(temp_pred_mat)['pred']\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading prediction data from '{temp_pred_mat}': {e}\")\n",
    "#         return\n",
    "\n",
    "#     # Calculate distances\n",
    "#     pre_dis1 = calculate_dis(0, 1, pre)\n",
    "#     pre_dis2 = calculate_dis(3, 5, pre)\n",
    "#     pre_dis3 = calculate_dis(16, 17, pre)\n",
    "#     pre_dis4 = calculate_dis(19, 20, pre)\n",
    "\n",
    "#     # Compute averages and standard deviations, ignoring NaN values\n",
    "#     pred_average = [\n",
    "#         np.nanmean(pre_dis1),\n",
    "#         np.nanmean(pre_dis2),\n",
    "#         np.nanmean(pre_dis3),\n",
    "#         np.nanmean(pre_dis4)\n",
    "#     ]\n",
    "#     pred_std = [\n",
    "#         np.nanstd(pre_dis1),\n",
    "#         np.nanstd(pre_dis2),\n",
    "#         np.nanstd(pre_dis3),\n",
    "#         np.nanstd(pre_dis4)\n",
    "#     ]\n",
    "\n",
    "#     # Plotting\n",
    "#     size = 4\n",
    "#     total_width, n = 0.8, 2\n",
    "#     x = np.arange(size)\n",
    "#     width = total_width / n\n",
    "#     x_shifted = x - (total_width - width) / 2  # Adjust the x position for better alignment\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.bar(x_shifted, ground_truth_average, width=width, yerr=ground_truth_std, label='GroundTruth', capsize=5)\n",
    "#     plt.bar(x_shifted + width, pred_average, width=width, yerr=pred_std, label='Prediction', capsize=5)\n",
    "#     plt.xticks(x + width / 2, labels)\n",
    "#     plt.xlabel('Segments')\n",
    "#     plt.ylabel('Distance (mm)')\n",
    "#     plt.title(os.path.basename(os.path.dirname(base_path)) +'_'+ os.path.basename(base_path))\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "\n",
    "#     # Save the plot\n",
    "#     save_name = os.path.join(save_path, f\"{os.path.basename(os.path.dirname(base_path))}_{os.path.basename(base_path)}.jpg\")\n",
    "#     try:\n",
    "#         plt.savefig(save_name)\n",
    "#         print(f\"Plot saved as '{save_name}'.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving plot '{save_name}': {e}\")\n",
    "    \n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from useful_files.sophie_check_dannce_mir_modif import dannce_valid\n",
    "records = [\n",
    "    {\n",
    "        'date_folder': date_folder.as_py(),  # Convert to string using as_py()\n",
    "        'rec_file': rec_file.as_py()         # Convert to string using as_py()\n",
    "    }\n",
    "    for date_folder, rec_file in zip(for_dannce_vis['date_folder'], for_dannce_vis['rec_file'])\n",
    "]\n",
    "\n",
    "# Iterate through the records and process each one sequentially\n",
    "# for record in records:\n",
    "#     base_path = f\"{base_folder}/{record['date_folder']}/{record['rec_file']}\"\n",
    "#     print(base_path)\n",
    "#     try:\n",
    "#         dannce_valid(base_path) #com_folder_name='COM/predict00', perform_jump_indices=True, perform_video_generation=False, perform_generate_com_video=False\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while processing {base_path}: {e}\")\n",
    "#         # Skip to the next record if an error occurs\n",
    "#         continue\n",
    "\n",
    "# #trying for parallel processing again:\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def process_record(record):\n",
    "    base_path = f\"{base_folder}/{record['date_folder']}/{record['rec_file']}\"\n",
    "    print(base_path)\n",
    "    try:\n",
    "        dannce_valid(base_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {base_path}: {e}\")\n",
    "        # Skip to the next record if an error occurs\n",
    "        pass\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_record, record) for record in records]\n",
    "    for future in as_completed(futures):\n",
    "        pass  # You can retrieve results or handle exceptions here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utlis.vis_valid_utlis.com_trag_updated import generate_dannce_vid_seq\n",
    "\n",
    "# # Assume base_folder is defined elsewhere in your script\n",
    "# # base_folder = '/path/to/your/base/folder'\n",
    "\n",
    "# records = [\n",
    "#     {\n",
    "#         'date_folder': date_folder.as_py(),  # Convert to string using as_py()\n",
    "#         'rec_file': rec_file.as_py()         # Convert to string using as_py()\n",
    "#     }\n",
    "#     for date_folder, rec_file in zip(for_dannce_vis['date_folder'], for_dannce_vis['rec_file'])\n",
    "# ]\n",
    "\n",
    "# # Iterate through the records and process each one sequentially\n",
    "# for record in records:\n",
    "#     base_path = f\"{base_folder}/{record['date_folder']}/{record['rec_file']}\"\n",
    "#     print(base_path)\n",
    "#     try:\n",
    "#         generate_dannce_vid_seq(base_path) #com_folder_name='COM/predict00', perform_jump_indices=True, perform_video_generation=False, perform_generate_com_video=False\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while processing {base_path}: {e}\")\n",
    "#         # Skip to the next record if an error occurs\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBOP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
