{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# -----------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------\n",
    "def load_data(hdf5_file_path, session_id=None):\n",
    "    \"\"\"Load HDF5 file and attach a session_id column.\"\"\"\n",
    "    df = pd.read_hdf(hdf5_file_path, key='df')\n",
    "    if session_id is not None:\n",
    "        df['session_id'] = session_id\n",
    "    return df\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    return v if norm < 1e-8 else v / norm\n",
    "\n",
    "def compute_euler_angles_from_row(row):\n",
    "    \"\"\"Compute head Euler angles (relative to body) for one frame.\"\"\"\n",
    "    def get_kp(idx):\n",
    "        return np.array([row[f'kp{idx}_x'], row[f'kp{idx}_y'], row[f'kp{idx}_z']])\n",
    "    \n",
    "    # Head coordinate frame (EarL=1, EarR=2, Snout=3)\n",
    "    earL = get_kp(1)\n",
    "    earR = get_kp(2)\n",
    "    snout = get_kp(3)\n",
    "    ear_mid = (earL + earR) / 2.0\n",
    "    head_x = normalize(snout - ear_mid)\n",
    "    temp_y = earR - earL\n",
    "    head_y = normalize(temp_y - np.dot(temp_y, head_x) * head_x)\n",
    "    head_z = normalize(np.cross(head_x, head_y))\n",
    "    \n",
    "    # Body coordinate frame (SpineF=4, SpineM=5, ShoulderL=12, ShoulderR=16)\n",
    "    spineF = get_kp(4)\n",
    "    spineM = get_kp(5)\n",
    "    shoulderL = get_kp(12)\n",
    "    shoulderR = get_kp(16)\n",
    "    body_x = normalize(spineF - spineM)\n",
    "    shoulder_vec = shoulderR - shoulderL\n",
    "    body_y = normalize(shoulder_vec - np.dot(shoulder_vec, body_x) * body_x)\n",
    "    body_z = normalize(np.cross(body_x, body_y))\n",
    "    \n",
    "    # Compute relative rotation from body to head.\n",
    "    R_body = np.column_stack((body_x, body_y, body_z))\n",
    "    R_head = np.column_stack((head_x, head_y, head_z))\n",
    "    R_rel = np.dot(R_body.T, R_head)\n",
    "    euler_angles = R.from_matrix(R_rel).as_euler('xyz', degrees=True)\n",
    "    return euler_angles\n",
    "\n",
    "def compute_all_euler_angles(df):\n",
    "    \"\"\"Compute Euler angles for all frames and add them as columns to df.\"\"\"\n",
    "    nframes = len(df)\n",
    "    yaw_vals   = np.empty(nframes)\n",
    "    pitch_vals = np.empty(nframes)\n",
    "    roll_vals  = np.empty(nframes)\n",
    "    for i in range(nframes):\n",
    "        row = df.iloc[i]\n",
    "        euler = compute_euler_angles_from_row(row)\n",
    "        yaw_vals[i]   = euler[0]\n",
    "        pitch_vals[i] = euler[1]\n",
    "        roll_vals[i]  = euler[2]\n",
    "    # Unwrap angles and convert to degrees.\n",
    "    yaw_vals = np.degrees(np.unwrap(np.radians(yaw_vals)))\n",
    "    pitch_vals = np.degrees(np.unwrap(np.radians(pitch_vals)))\n",
    "    roll_vals = np.degrees(np.unwrap(np.radians(roll_vals)))\n",
    "    df['Yaw'] = yaw_vals\n",
    "    df['Pitch'] = pitch_vals\n",
    "    df['Roll'] = roll_vals\n",
    "    return yaw_vals, pitch_vals, roll_vals\n",
    "\n",
    "def get_neural_columns(df, prefix='dF_F_roi'):\n",
    "    \"\"\"Retrieve neural signal columns based on prefix.\"\"\"\n",
    "    return [col for col in df.columns if col.startswith(prefix)]\n",
    "\n",
    "def exclude_neurons(df, neural_columns, exclude_indices, outlier_sigma=3):\n",
    "    \"\"\"\n",
    "    Exclude neurons based on outlier criteria and manual indices.\n",
    "    \"\"\"\n",
    "    # Outlier exclusion based on max activity.\n",
    "    neuron_max_vals = {neuron: df[neuron].max() for neuron in neural_columns}\n",
    "    max_array = np.array(list(neuron_max_vals.values()))\n",
    "    mean_max = np.mean(max_array)\n",
    "    std_max = np.std(max_array)\n",
    "    threshold = mean_max + outlier_sigma * std_max\n",
    "    outlier_neurons = [neuron for neuron, val in neuron_max_vals.items() if val > threshold]\n",
    "    \n",
    "    # Manual exclusion.\n",
    "    manual_excluded_neurons = [f'dF_F_roi{i}' for i in exclude_indices]\n",
    "    clean_neural_columns = [\n",
    "        neuron for neuron in neural_columns \n",
    "        if (neuron not in outlier_neurons and neuron not in manual_excluded_neurons)\n",
    "    ]\n",
    "    return clean_neural_columns\n",
    "\n",
    "def compute_tuning_curve(angle_vals, neural_vals, nbins=20):\n",
    "    \"\"\"\n",
    "    Bin angle values and compute the mean neural response for each bin.\n",
    "    \"\"\"\n",
    "    bins = np.linspace(np.min(angle_vals), np.max(angle_vals), nbins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2.0\n",
    "    avg_response = np.zeros(nbins)\n",
    "    for i in range(nbins):\n",
    "        mask = (angle_vals >= bins[i]) & (angle_vals < bins[i+1])\n",
    "        avg_response[i] = np.mean(neural_vals[mask]) if np.any(mask) else np.nan\n",
    "    return bin_centers, avg_response\n",
    "\n",
    "def select_best_neuron(df, angle_vals, clean_neural_columns, nbins=20):\n",
    "    \"\"\"\n",
    "    Select the neuron with the highest tuning curve variance for a given angle.\n",
    "    \"\"\"\n",
    "    best_metric = -np.inf\n",
    "    selected_neuron = None\n",
    "    for neuron in clean_neural_columns:\n",
    "        neural_vals = df[neuron].values\n",
    "        _, tuning = compute_tuning_curve(angle_vals, neural_vals, nbins)\n",
    "        metric = np.nanvar(tuning)\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            selected_neuron = neuron\n",
    "    return selected_neuron\n",
    "\n",
    "def plot_tuning_curves(df, yaw_vals, pitch_vals, roll_vals, clean_neural_columns, best_neurons, nbins=20):\n",
    "    \"\"\"Plot tuning curves for Yaw, Pitch, and Roll.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 12), sharex=False)\n",
    "    angle_names = ['Yaw', 'Pitch', 'Roll']\n",
    "    angle_vals_list = [yaw_vals, pitch_vals, roll_vals]\n",
    "    \n",
    "    for ax, angle_name, angle_vals in zip(axes, angle_names, angle_vals_list):\n",
    "        # Plot all neurons in background.\n",
    "        for neuron in clean_neural_columns:\n",
    "            neural_vals = df[neuron].values\n",
    "            centers, tuning = compute_tuning_curve(angle_vals, neural_vals, nbins)\n",
    "            ax.plot(centers, tuning, linestyle='-', color='lightgray', alpha=0.5, zorder=1)\n",
    "        \n",
    "        # Highlight best neuron.\n",
    "        neuron = best_neurons[angle_name]\n",
    "        neural_vals = df[neuron].values\n",
    "        centers, tuning = compute_tuning_curve(angle_vals, neural_vals, nbins)\n",
    "        ax.plot(centers, tuning, linestyle='-', color='blue', lw=2.5, label=f'Best: {neuron}', zorder=2)\n",
    "        ax.set_title(f'Tuning Curves for Head {angle_name}')\n",
    "        ax.set_xlabel(f'{angle_name} (deg)')\n",
    "        ax.set_ylabel('Neural Response (dF/F)')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def clustering_analysis(df, clean_neural_columns, num_clusters=2):\n",
    "    \"\"\"\n",
    "    Cluster neurons based on their activity and plot cluster analysis views.\n",
    "    \"\"\"\n",
    "    # Reset index without dropping so that 'timestamp_ms_mini' is preserved as a column.\n",
    "    df_new = df.copy().reset_index()\n",
    "    time = df_new['timestamp_ms_mini']\n",
    "    neuron_activity = df_new[clean_neural_columns].values.T  # shape: (neurons, timepoints)\n",
    "    \n",
    "    # Drop low-variance neurons.\n",
    "    neuron_variances = np.var(neuron_activity, axis=1)\n",
    "    threshold = np.percentile(neuron_variances, 5)\n",
    "    high_variance_indices = neuron_variances > threshold\n",
    "    neuron_activity_filtered = neuron_activity[high_variance_indices, :]\n",
    "    filtered_neural_columns = [col for i, col in enumerate(clean_neural_columns) if high_variance_indices[i]]\n",
    "    \n",
    "    # Z-score normalization.\n",
    "    neuron_activity_normalized = zscore(neuron_activity_filtered, axis=1)\n",
    "    Z = linkage(neuron_activity_normalized, method='average', metric='correlation')\n",
    "    cluster_labels = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Compute cluster mean activity.\n",
    "    cluster_means = []\n",
    "    for cluster_id in range(1, num_clusters + 1):\n",
    "        cluster_neurons = neuron_activity_normalized[cluster_labels == cluster_id, :]\n",
    "        cluster_mean = cluster_neurons.mean(axis=0)\n",
    "        cluster_means.append(cluster_mean)\n",
    "    cluster_means = np.array(cluster_means)\n",
    "    \n",
    "    # Plot cluster analysis results.\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    for cluster_id in range(num_clusters):\n",
    "        row_offset = cluster_id * 3\n",
    "        # 3D COM Trajectory.\n",
    "        ax1 = fig.add_subplot(2, 3, row_offset + 1, projection='3d')\n",
    "        sc1 = ax1.scatter(df_new['com_x'], df_new['com_y'], df_new['com_z'],\n",
    "                          c=cluster_means[cluster_id, :len(df_new)], cmap='hot', marker='o')\n",
    "        ax1.set_title(f'3D COM Trajectory (Cluster {cluster_id + 1})')\n",
    "        ax1.set_xlabel('X Position')\n",
    "        ax1.set_ylabel('Y Position')\n",
    "        ax1.set_zlabel('Z Position')\n",
    "        fig.colorbar(sc1, ax=ax1, pad=0.1, label='Mean Neural Activity (ÎF/F)')\n",
    "        \n",
    "        # XY Plane Projection.\n",
    "        ax2 = fig.add_subplot(2, 3, row_offset + 2)\n",
    "        sc2 = ax2.scatter(df_new['com_x'], df_new['com_y'],\n",
    "                          c=cluster_means[cluster_id, :len(df_new)], cmap='hot', marker='o')\n",
    "        ax2.set_title(f'XY Plane Projection (Cluster {cluster_id + 1})')\n",
    "        ax2.set_xlabel('X Position')\n",
    "        ax2.set_ylabel('Y Position')\n",
    "        fig.colorbar(sc2, ax=ax2, label='Mean Neural Activity (ÎF/F)')\n",
    "        \n",
    "        # Z-Only View.\n",
    "        ax3 = fig.add_subplot(2, 3, row_offset + 3)\n",
    "        sc3 = ax3.scatter(time, df_new['com_z'],\n",
    "                          c=cluster_means[cluster_id, :len(df_new)], cmap='hot', marker='o')\n",
    "        ax3.set_title(f'Z-Only View (Cluster {cluster_id + 1})')\n",
    "        ax3.set_xlabel('Time (ms)')\n",
    "        ax3.set_ylabel('Z Position')\n",
    "        fig.colorbar(sc3, ax=ax3, label='Mean Neural Activity (ÎF/F)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return cluster_labels, filtered_neural_columns\n",
    "\n",
    "# -----------------------------\n",
    "# Pipeline for a Single Session\n",
    "# -----------------------------\n",
    "def run_single_session(session_params):\n",
    "    \"\"\"\n",
    "    Run the analysis pipeline for a single session.\n",
    "    \n",
    "    session_params should be a dictionary with keys:\n",
    "      - 'file_path'\n",
    "      - 'session_id'\n",
    "      - 'exclude_indices': list of neuron indices to exclude manually\n",
    "    \"\"\"\n",
    "    print(f\"Processing session: {session_params.get('session_id', 'unknown')}\")\n",
    "    # Load data.\n",
    "    df = load_data(session_params['file_path'], session_id=session_params.get('session_id'))\n",
    "    \n",
    "    # Compute Euler angles.\n",
    "    yaw_vals, pitch_vals, roll_vals = compute_all_euler_angles(df)\n",
    "    \n",
    "    # Retrieve and clean neural columns.\n",
    "    neural_columns = get_neural_columns(df)\n",
    "    clean_neural_columns = exclude_neurons(df, neural_columns, session_params.get('exclude_indices', []))\n",
    "    \n",
    "    # Select the best neuron for each angle.\n",
    "    best_neurons = {\n",
    "        'Yaw': select_best_neuron(df, yaw_vals, clean_neural_columns),\n",
    "        'Pitch': select_best_neuron(df, pitch_vals, clean_neural_columns),\n",
    "        'Roll': select_best_neuron(df, roll_vals, clean_neural_columns)\n",
    "    }\n",
    "    \n",
    "    # Plot tuning curves.\n",
    "    plot_tuning_curves(df, yaw_vals, pitch_vals, roll_vals, clean_neural_columns, best_neurons)\n",
    "    \n",
    "    # Perform clustering analysis.\n",
    "    cluster_labels, filtered_neural_columns = clustering_analysis(df, clean_neural_columns)\n",
    "    \n",
    "    return {\n",
    "        'session_id': session_params.get('session_id'),\n",
    "        'best_neurons': best_neurons,\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'filtered_neural_columns': filtered_neural_columns\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Main: Run Sessions with Exclusion from JSON\n",
    "# -----------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Base path to prepend to session relative paths.\n",
    "    base_path = \"/data/big_rim/rsync_dcc_sum/Oct3V1\"\n",
    "    \n",
    "    # Load exclusion indices from JSON.\n",
    "    exclude_file = \"/home/lq53/mir_repos/BBOP/random_tests/25feb_more_corr_explo/neuro_exclude.json\"\n",
    "    with open(exclude_file, \"r\") as f:\n",
    "        exclude_dict = json.load(f)\n",
    "    \n",
    "    # List of session relative paths.\n",
    "    recordings = [\n",
    "        '2024_10_25/20241002PMCr2_15_42',\n",
    "        '2024_10_25/20241002PMCr2_17_05',\n",
    "        '2024_10_14/20240916v1r1_16_37',\n",
    "        '2024_10_14/20240916v1r1_16_53',\n",
    "        '2024_10_14/20240916v1r2_14_30',\n",
    "        '2024_10_14/20240916v1r2_15_58',\n",
    "        '2024_11_01/20240910V1r_AO_12_50',\n",
    "        '2024_11_06/20241015pmcr2_16_53'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for rec in recordings:\n",
    "        # Construct the session directory and the full file path.\n",
    "        session_dir = f\"{base_path}/{rec}\"\n",
    "        file_path = f\"{session_dir}/MIR_Aligned/aligned_predictions_with_ca_and_dF_F.h5\"\n",
    "        # Compute session id by replacing \"/\" with \"_\".\n",
    "        session_id = rec.replace(\"/\", \"_\")\n",
    "        # Get exclude indices from JSON if present; otherwise, use an empty list.\n",
    "        exclude_indices = exclude_dict.get(session_dir, [])\n",
    "        \n",
    "        session_params = {\n",
    "            'file_path': file_path,\n",
    "            'session_id': session_id,\n",
    "            'exclude_indices': exclude_indices\n",
    "        }\n",
    "        print(f\"\\nRunning analysis for session: {session_id}\")\n",
    "        res = run_single_session(session_params)\n",
    "        results.append(res)\n",
    "    \n",
    "    print(\"Session analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.ndimage import binary_erosion\n",
    "from scipy.sparse import csc_matrix\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "# Import your miniscope utilities.\n",
    "from utlis.Ca_tools.roi_spike_vis_utlis import load_minian_data, calculate_dff, overlay_roi_edges_exclude\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Find Miniscope Path from Mapping\n",
    "# -----------------------------\n",
    "def find_minian_path_for_recording(recording, mapping_data):\n",
    "    \"\"\"\n",
    "    Given a recording identifier (e.g. '2024_10_25/20241002PMCr2_15_42'),\n",
    "    search mapping_data (from mini_to_rec_mapping.json) for a miniscope path\n",
    "    whose key or its associated \"rec_path\" field contains the session ID.\n",
    "    \n",
    "    Returns the miniscope path if found, else None.\n",
    "    \"\"\"\n",
    "    session_id = recording.split('/')[-1]\n",
    "    for mini_path, mapping in mapping_data.items():\n",
    "        rec_field = mapping.get(\"rec_path\") or \"\"\n",
    "        if session_id in mini_path or session_id in rec_field:\n",
    "            return mini_path\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Check if neuron should be excluded\n",
    "# -----------------------------\n",
    "def is_excluded(neuron_name, exclusions):\n",
    "    \"\"\"\n",
    "    Returns True if the neuron (e.g. 'dF_F_roi5') is in the exclusions list.\n",
    "    The exclusions list may contain integers or strings (like 5, '5', or \"dF_F_roi5\").\n",
    "    \"\"\"\n",
    "    try:\n",
    "        roi_idx = int(neuron_name.replace(\"dF_F_roi\", \"\"))\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return (roi_idx in exclusions) or (str(roi_idx) in exclusions) or (neuron_name in exclusions)\n",
    "\n",
    "# -----------------------------\n",
    "# Function: Cluster Rec Data & Overlay Miniscope ROIs with ROI labels\n",
    "# -----------------------------\n",
    "def overlay_clustered_roi_edges_from_rec_and_minian(rec_file_path, minian_path,\n",
    "                                                    clean_neural_columns=None,\n",
    "                                                    exclusions=None,\n",
    "                                                    num_clusters=2):\n",
    "    \"\"\"\n",
    "    Load rec data from rec_file_path to perform clustering on neural activity,\n",
    "    _excluding_ neurons specified in 'exclusions', then load the miniscope data from\n",
    "    minian_path and overlay ROI edges (red for cluster 1, blue for cluster 2) on the max projection.\n",
    "    Additionally, the function labels each ROI at its centroid.\n",
    "    \n",
    "    Parameters:\n",
    "      rec_file_path (str): Full path to the rec HDF5 file.\n",
    "      minian_path (str): Directory path to the miniscope data.\n",
    "      clean_neural_columns (list, optional): List of neural column names to use.\n",
    "          If None, all columns starting with \"dF_F_roi\" will be used.\n",
    "      exclusions (list, optional): List of ROI names or indices to exclude \n",
    "          (e.g., [5, 17] or [\"dF_F_roi5\", \"dF_F_roi17\"]). If None, no exclusions are applied.\n",
    "      num_clusters (int, optional): Number of clusters to form (default is 2).\n",
    "    \n",
    "    Returns:\n",
    "      roi_cluster_mapping (dict): Mapping from ROI name to cluster label (only for non-excluded neurons).\n",
    "      rec_cluster_labels (np.array): Cluster labels from the rec data (for the filtered neurons).\n",
    "    \"\"\"\n",
    "    from scipy.ndimage import binary_erosion  # Ensure binary_erosion is imported\n",
    "    from scipy.stats import zscore\n",
    "    from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "    if exclusions is None:\n",
    "        exclusions = []\n",
    "    \n",
    "    # --- 1. Load and Process Rec Data (with exclusions) ---\n",
    "    df = pd.read_hdf(rec_file_path, key='df').reset_index()\n",
    "    \n",
    "    # Derive neural columns if not provided.\n",
    "    if clean_neural_columns is None:\n",
    "        neural_columns = [col for col in df.columns if col.startswith(\"dF_F_roi\")]\n",
    "    else:\n",
    "        neural_columns = clean_neural_columns\n",
    "    \n",
    "    # Exclude neurons before clustering using the helper function.\n",
    "    clean_neural_columns = [col for col in neural_columns if not is_excluded(col, exclusions)]\n",
    "    if len(clean_neural_columns) == 0:\n",
    "        raise ValueError(\"No neurons left after applying exclusions!\")\n",
    "    \n",
    "    # Extract neural activity (transpose so shape becomes: neurons x timepoints).\n",
    "    neuron_activity = df[clean_neural_columns].values.T\n",
    "    \n",
    "    # Drop low-variance neurons.\n",
    "    neuron_variances = np.var(neuron_activity, axis=1)\n",
    "    threshold = np.percentile(neuron_variances, 5)\n",
    "    high_variance_mask = neuron_variances > threshold\n",
    "    neuron_activity_filtered = neuron_activity[high_variance_mask, :]\n",
    "    filtered_neural_columns = [col for i, col in enumerate(clean_neural_columns) if high_variance_mask[i]]\n",
    "    \n",
    "    # Z-score normalization.\n",
    "    neuron_activity_normalized = zscore(neuron_activity_filtered, axis=1)\n",
    "    \n",
    "    # Hierarchical clustering.\n",
    "    Z = linkage(neuron_activity_normalized, method='average', metric='correlation')\n",
    "    rec_cluster_labels = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Build mapping from ROI name to cluster label.\n",
    "    roi_cluster_mapping = {roi: cl for roi, cl in zip(filtered_neural_columns, rec_cluster_labels)}\n",
    "    \n",
    "    # --- 2. Load Miniscope Data for the Overlay ---\n",
    "    mini_timestamps = os.path.join(minian_path, 'timeStamps.csv')\n",
    "    data, ts = load_minian_data(minian_path, mini_timestamps)\n",
    "    \n",
    "    try:\n",
    "        max_proj = data['max_proj'].values\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Error retrieving max projection from miniscope data: \" + str(e))\n",
    "    \n",
    "    # Ensure ROI footprints (data['A']) are in dense format.\n",
    "    if hasattr(data['A'], 'toarray'):\n",
    "        A_dense = data['A'].toarray()\n",
    "    elif hasattr(data['A'], 'values'):\n",
    "        A_dense = data['A'].values\n",
    "    else:\n",
    "        A_dense = data['A']\n",
    "    \n",
    "    # Create mapping from ROI name to index in miniscope data.\n",
    "    # We assume the miniscope ROI names are \"dF_F_roi0\", \"dF_F_roi1\", ... in order.\n",
    "    roi_mapping = {f\"dF_F_roi{i}\": i for i in range(A_dense.shape[0])}\n",
    "    \n",
    "    # --- 3. Build Cluster Overlays (Red for Cluster 1, Blue for Cluster 2) ---\n",
    "    cluster1_overlay = np.zeros_like(max_proj, dtype=float)\n",
    "    cluster2_overlay = np.zeros_like(max_proj, dtype=float)\n",
    "    \n",
    "    for roi_name, cl in roi_cluster_mapping.items():\n",
    "        if roi_name not in roi_mapping:\n",
    "            print(f\"Warning: ROI {roi_name} not found in miniscope data, skipping.\")\n",
    "            continue\n",
    "        i = roi_mapping[roi_name]\n",
    "        # Construct the ROI mask.\n",
    "        if A_dense.ndim == 3:\n",
    "            roi_mask = A_dense[i] > 0\n",
    "        else:\n",
    "            roi_mask = A_dense[i].reshape(max_proj.shape) > 0\n",
    "        # Compute ROI edge (mask XOR its binary erosion).\n",
    "        roi_edge = roi_mask ^ binary_erosion(roi_mask)\n",
    "        # Add the edge to the corresponding cluster overlay.\n",
    "        if cl == 1:\n",
    "            cluster1_overlay += roi_edge.astype(float)\n",
    "        elif cl == 2:\n",
    "            cluster2_overlay += roi_edge.astype(float)\n",
    "    \n",
    "    # Clip overlay values to [0, 1].\n",
    "    cluster1_overlay = np.clip(cluster1_overlay, 0, 1)\n",
    "    cluster2_overlay = np.clip(cluster2_overlay, 0, 1)\n",
    "    \n",
    "    # --- 4. Plot the Overlays with ROI Labels ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # Display the max projection without specifying a colormap to mimic your original look.\n",
    "    plt.imshow(max_proj, interpolation='nearest')\n",
    "    plt.imshow(cluster1_overlay, cmap='Reds', alpha=0.3, interpolation='nearest')\n",
    "    plt.imshow(cluster2_overlay, cmap='Blues', alpha=0.3, interpolation='nearest')\n",
    "    \n",
    "    # Label each ROI at its centroid.\n",
    "    for roi_name, cl in roi_cluster_mapping.items():\n",
    "        if roi_name not in roi_mapping:\n",
    "            continue\n",
    "        idx = roi_mapping[roi_name]\n",
    "        # Construct the ROI mask.\n",
    "        if A_dense.ndim == 3:\n",
    "            roi_mask = A_dense[idx] > 0\n",
    "        else:\n",
    "            roi_mask = A_dense[idx].reshape(max_proj.shape) > 0\n",
    "        coords = np.argwhere(roi_mask)\n",
    "        if coords.size > 0:\n",
    "            centroid = coords.mean(axis=0)\n",
    "            # Use red for cluster 1, blue for cluster 2.\n",
    "            cluster_color = 'red' if cl == 1 else 'blue'\n",
    "            # Extract the numeric part of the ROI name.\n",
    "            roi_label = roi_name.replace(\"dF_F_roi\", \"\")\n",
    "            plt.text(centroid[1], centroid[0] + 20, roi_label, color=cluster_color,\n",
    "                     fontsize=10, ha='center', va='center')\n",
    "    \n",
    "    plt.title('Max Projection with Clustered ROI Edges Overlay (Exclusions Applied)')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return roi_cluster_mapping, rec_cluster_labels\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main Script: Example Usage\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # Define the base recor-+-ding path and list of recordings.\n",
    "    base_path = \"/data/big_rim/rsync_dcc_sum/Oct3V1\"\n",
    "    # recordings = [\n",
    "    #     '2024_10_25/20241002PMCr2_15_42',\n",
    "    #     '2024_10_25/20241002PMCr2_17_05',\n",
    "    #     '2024_11_06/20241015pmcr2_16_53'\n",
    "    # ]\n",
    "    recordings = [\n",
    "        '2024_10_25/20241002PMCr2_15_42',\n",
    "        '2024_10_25/20241002PMCr2_17_05',\n",
    "        '2024_10_14/20240916v1r1_16_37',\n",
    "        '2024_10_14/20240916v1r1_16_53',\n",
    "        '2024_10_14/20240916v1r2_14_30',\n",
    "        '2024_10_14/20240916v1r2_15_58',\n",
    "        '2024_11_01/20240910V1r_AO_12_50',\n",
    "        '2024_11_06/20241015pmcr2_16_53'\n",
    "    ]\n",
    "    \n",
    "    # Load the mapping JSON.\n",
    "    mapping_file = \"/home/lq53/mir_repos/BBOP/random_tests/25feb_more_corr_explo/mini_to_rec_mapping.json\"\n",
    "    with open(mapping_file, \"r\") as f:\n",
    "        mapping_data = json.load(f)\n",
    "    \n",
    "    # Load the neuron exclusion JSON.\n",
    "    exclude_file = \"/home/lq53/mir_repos/BBOP/random_tests/25feb_more_corr_explo/neuro_exclude.json\"\n",
    "    with open(exclude_file, \"r\") as f:\n",
    "        neuro_exclude = json.load(f)\n",
    "    \n",
    "    # Loop over each recording.\n",
    "    for rec in recordings:\n",
    "        rec_path = os.path.join(base_path, rec)\n",
    "        print(f\"\\nProcessing recording: {rec_path}\")\n",
    "        \n",
    "        # Find corresponding miniscope path.\n",
    "        minian_path = find_minian_path_for_recording(rec, mapping_data)\n",
    "        if minian_path is None:\n",
    "            print(f\"  Miniscope path not found for {rec}\")\n",
    "            continue\n",
    "        print(f\"  Miniscope path found: {minian_path}\")\n",
    "        \n",
    "        # Construct the rec file path.\n",
    "        rec_file_path = os.path.join(rec_path, \"MIR_Aligned\", \"aligned_predictions_with_ca_and_dF_F.h5\")\n",
    "        \n",
    "        # Get exclusions for this recording from the exclusion JSON.\n",
    "        exclusions = neuro_exclude.get(rec_path, [])\n",
    "        \n",
    "        # --- Optionally: Overlay ROI edges with exclusions using miniscope data only ---\n",
    "        try:\n",
    "            mini_timestamps = os.path.join(minian_path, 'timeStamps.csv')\n",
    "            data, ts = load_minian_data(minian_path, mini_timestamps)\n",
    "            overlay_roi_edges_exclude(data, data['max_proj'].values, exclusions)\n",
    "            print(\"  ROI edges overlaid with exclusions (miniscope only).\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error overlaying ROI edges with exclusions: {e}\")\n",
    "        \n",
    "        # --- Perform rec data clustering & overlay clustered ROI edges with ROI labels ---\n",
    "        try:\n",
    "            roi_cluster_mapping, rec_cluster_labels = overlay_clustered_roi_edges_from_rec_and_minian(\n",
    "                rec_file_path, minian_path, clean_neural_columns=None, exclusions=exclusions, num_clusters=2)\n",
    "            print(\"  Auto-clustered ROI edges overlaid with ROI labels.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error overlaying clustered ROI edges: {e}\")\n",
    "    \n",
    "    print(\"Processing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--aggregate]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/1001/jupyter/runtime/kernel-v3ce82c3530ce1049b220ef29f34e6aa2abd577d74.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lq53/miniconda3/envs/bbop241209/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbop241209",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
