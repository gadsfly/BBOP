{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Data Processing Pipeline\n",
    "\n",
    "**Author:** Mir Qi  \n",
    "**Last Updated:** November 2024  \n",
    "\n",
    "## üìã Pipeline Overview\n",
    "\n",
    "This notebook walks through the complete preprocessing pipeline for multi-camera behavioral recordings:\n",
    "\n",
    "1. **Scan & Log** - Index all recording sessions\n",
    "2. **Load & Filter**  - Load metadata and select sessions\n",
    "3. **Calib Generat5e** - Generate calibration (raw data only)\n",
    "4. **Camera Sync**  - Align multi-camera timestamps\n",
    "5. **COM Prediction**  - Coarse animal localization\n",
    "6. **COM Validation** - Quality control visualization\n",
    "7. **sDANNCE Prediction**  - Full 3D pose estimation\n",
    "8. **sDANNCE Validation** - Pose quality metrics\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "**Required:**\n",
    "- Access to Duke cluster/or local GPU\n",
    "- Conda environment: `sdannce`\n",
    "- Calibration files in recording directories\n",
    "- Multi-camera video recordings with synchronized timestamps\n",
    "\n",
    "**Python Environment:**\n",
    "```bash\n",
    "conda activate bbop\n",
    "```\n",
    "\n",
    "**Data Structure:**\n",
    "```\n",
    "base_folder/\n",
    "‚îú‚îÄ‚îÄ YYYY_MM_DD/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ session_name/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ videos/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Camera1/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frametimes.mat\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 0.mp4\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Camera2/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ *labe3d_dannce.mat/  # calibration folders\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ folder_log.parquet  # generated by pipeline\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Scan Folders & Generate Logs\n",
    "\n",
    "### What this does:\n",
    "- Scans the base folder for all recording sessions\n",
    "- Generates/updates `folder_log.parquet` files with processing status\n",
    "- Creates a database of session metadata for filtering\n",
    "\n",
    "\n",
    "\n",
    " (you may need to manually cerate a paret folder in the base path, which will be revised in the future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log for ZIcI1_0mW saved at /data/big_rim/rsync_dcc_sum/bbop_demo/25Nov/2025_10_31/ZIcI1_0mW/folder_log.parquet\n",
      "\n",
      "‚úì Scanning complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from status_fields.status_fields_config_oct3v1_brws_250525 import STATUS_FIELDS_CONFIG\n",
    "from utlis.scan_engine_utlis.scan_eng_big_utlis import log_folder_to_parquet_sep\n",
    "\n",
    "# Configuration\n",
    "base_folder = \"/data/big_rim/rsync_dcc_sum/bbop_demo/25Nov\"\n",
    "failed_paths_file = None  # Optional: path to txt file with failed sessions\n",
    "force_rescan_rec_files = []  # Optional: [('date', 'session_name')] to force rescan\n",
    "rescan_threshold_days = 0.000000001  # Rescan if modified within this many days\n",
    "\n",
    "log_folder_to_parquet_sep(base_folder, failed_paths_file, STATUS_FIELDS_CONFIG,\n",
    "                            force_rescan_rec_files=force_rescan_rec_files,\n",
    "                            rescan_threshold_days=rescan_threshold_days)\n",
    "\n",
    "print(\"\\n‚úì Scanning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Data\n",
    "\n",
    "### What this does:\n",
    "- Reads all `folder_log.parquet` files from scanned sessions\n",
    "- Combines them into a single PyArrow table for efficient filtering\n",
    "\n",
    "### Data structure:\n",
    "Each row represents one recording session with columns:\n",
    "- `rec_file`: Session directory name\n",
    "- `date_folder`: Date folder (YYYY_MM_DD)\n",
    "- `rec_path`: Full path to session\n",
    "- `mir_generate_param`: Frist step of parameters generated? (0/1)\n",
    "- `sync`: Camera synchronization done? (0/1)\n",
    "- `com`: COM prediction done? (0/1)\n",
    "- `com_vis`: COM validation done? (0/1)\n",
    "- `dannce`: DANNCE prediction done? (0/1)\n",
    "- `dannce_vis`: DANNCE validation done? (0/1)\n",
    "- `social`: Social interaction session? (0/1)\n",
    "- `scan_time`: Last scan timestamp\n",
    "- `calib_files`: Available calibration folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 1 sessions\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('../'))\n",
    "from utlis.scan_engine_utlis.scan_engine_utlis import read_all_parquet_files\n",
    "\n",
    "# Load all session metadata\n",
    "all_df = read_all_parquet_files(base_folder)\n",
    "\n",
    "print(f\"‚úì Loaded {len(all_df)} sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mir_generate_param sync mini_6cam_map dropf_handle com com_vis social  \\\n",
      "0                  0    0             0            0   0       0      0   \n",
      "\n",
      "  miniscope test after_oxytocin before_oxytocin dannce dannce_vis  \\\n",
      "0         0    0              0               1      0          0   \n",
      "\n",
      "  mini_rec_sync mini_rec_sync_com   rec_file                   scan_time  \\\n",
      "0             0                 0  ZIcI1_0mW  2025-11-04T22:40:07.663238   \n",
      "\n",
      "                                            rec_path date_folder  \\\n",
      "0  /data/big_rim/rsync_dcc_sum/bbop_demo/25Nov/20...  2025_10_31   \n",
      "\n",
      "      calib_files  \n",
      "0  [calib_before]  \n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas for viewing (PyArrow table underneath for filtering)\n",
    "print(all_df.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mir_generate_param',\n",
       " 'sync',\n",
       " 'mini_6cam_map',\n",
       " 'dropf_handle',\n",
       " 'com',\n",
       " 'com_vis',\n",
       " 'social',\n",
       " 'miniscope',\n",
       " 'test',\n",
       " 'after_oxytocin',\n",
       " 'before_oxytocin',\n",
       " 'dannce',\n",
       " 'dannce_vis',\n",
       " 'mini_rec_sync',\n",
       " 'mini_rec_sync_com',\n",
       " 'rec_file',\n",
       " 'scan_time',\n",
       " 'rec_path',\n",
       " 'date_folder',\n",
       " 'calib_files']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View all available columns\n",
    "all_df.to_pandas().columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Filter Sessions\n",
    "\n",
    "### What this does:\n",
    "- Filters sessions based on processing status flags\n",
    "- Selects only sessions that need the next processing step\n",
    "- Uses PyArrow compute for efficient filtering on large datasets\n",
    "\n",
    "### Important notes:\n",
    "- **Values are strings**: Use `'0'` and `'1'`, not integers `0` and `1`\n",
    "- **Multiple conditions**: combined to select.\n",
    "- **Common filters** listed at end of notebook for reference\n",
    "\n",
    "### Example workflow:\n",
    "1. Filter for `mir_generate_param=0` ‚Üí run mir_generate_param \n",
    "2. Filter for `sync=0` ‚Üí run sync \n",
    "2. Filter for `com=0` ‚Üí run COM prediction\n",
    "3. Filter for `com=1, com_vis=0` ‚Üí run COM validation\n",
    "4. Continue through pipeline...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Filtered: 1 sessions match criteria\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.compute as pc\n",
    "from functools import reduce\n",
    "\n",
    "table = all_df\n",
    "\n",
    "# Example: Find social sessions that need COM validation\n",
    "conditions = [\n",
    "    pc.equal(table['mir_generate_param'], '0'),\n",
    "    pc.equal(table['social'], '0'),\n",
    "    pc.equal(table['sync'], '0'),\n",
    "    pc.equal(table['com'], '0'),\n",
    "    pc.equal(table['com_vis'], '0'),\n",
    "]\n",
    "\n",
    "filter_mask = reduce(pc.and_, conditions)\n",
    "filtered_table = table.filter(filter_mask)\n",
    "\n",
    "print(f\"‚úì Filtered: {len(filtered_table)} sessions match criteria\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View filtered session paths:\n",
    "\n",
    "**Why this is useful:**\n",
    "- Click paths to navigate to session directories\n",
    "- Verify correct sessions are selected before processing\n",
    "- Quick visual check of what will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions to process:\n",
      "\n",
      " 1. /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_1mW\n",
      " 2. /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_00mW\n",
      " 3. /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_0mW\n",
      " 4. /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_10mW\n",
      " 5. /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_5mW\n",
      "\n",
      "Total: 5 sessions\n"
     ]
    }
   ],
   "source": [
    "rec_paths = filtered_table[\"rec_path\"].to_pylist()\n",
    "\n",
    "print(\"Sessions to process:\\n\")\n",
    "for i, path in enumerate(rec_paths, 1):\n",
    "    path_str = path[0] if isinstance(path, list) else path\n",
    "    print(f\"{i:2d}. {path_str}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(rec_paths)} sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Generate Calibration File\n",
    "\n",
    "### When to run:\n",
    "**Only for raw data** - Filter: `mir_generate_param=0`\n",
    "\n",
    "### What this does:\n",
    "- Generates mirror calibration parameters for multi-camera setup\n",
    "- Uses specified calibration folder (checkerboard-based calibration)\n",
    "- Creates configuration files needed for downstream pose estimation\n",
    "- Updates status flag to `mir_generate_param=1` upon completion\n",
    "\n",
    "### Calibration options:\n",
    "Common calibration folder names:\n",
    "- `calib_before_newintrinsics` - Standard calibration\n",
    "- `calib_after` - Post-adjustment calibration\n",
    "- `calib_extrinsics_fixed` - Fixed extrinsics calibration\n",
    "\n",
    "### Troubleshooting:\n",
    "- **\"Calibration folder not found\"**: Check that specified calibration exists in session directory, you can use your customized subdirectories, the defualt is `/calib_before`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using calibration: calib_before\n",
      "\n",
      "Found 6 calibration files.\n",
      "Frame count: 18000\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam1_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam2_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam3_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam4_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam5_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam6_params.mat\n",
      "Data saved to /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_1mW/2025_10_31_ZIcI1_1mW_calib_before_label3d_dannce.mat\n",
      "mir_generate_param ran successfully.\n",
      "Updated Parquet file at /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_1mW/folder_log.parquet with new status.\n",
      "Found 6 calibration files.\n",
      "Frame count: 18000\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam1_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam2_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam3_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam4_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam5_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam6_params.mat\n",
      "Data saved to /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_00mW/2025_10_31_ZIcI1_00mW_calib_before_label3d_dannce.mat\n",
      "mir_generate_param ran successfully.\n",
      "Updated Parquet file at /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_00mW/folder_log.parquet with new status.\n",
      "Found 6 calibration files.\n",
      "Frame count: 18000\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam1_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam2_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam3_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam4_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam5_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam6_params.mat\n",
      "Data saved to /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_0mW/2025_10_31_ZIcI1_0mW_calib_before_label3d_dannce.mat\n",
      "mir_generate_param ran successfully.\n",
      "Updated Parquet file at /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_0mW/folder_log.parquet with new status.\n",
      "Found 6 calibration files.\n",
      "Frame count: 18000\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam1_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam2_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam3_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam4_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam5_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam6_params.mat\n",
      "Data saved to /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_10mW/2025_10_31_ZIcI1_10mW_calib_before_label3d_dannce.mat\n",
      "mir_generate_param ran successfully.\n",
      "Updated Parquet file at /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_10mW/folder_log.parquet with new status.\n",
      "Found 6 calibration files.\n",
      "Frame count: 18000\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam1_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam2_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam3_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam4_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam5_params.mat\n",
      "Processed /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/calib_before/hires_cam6_params.mat\n",
      "Data saved to /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_5mW/2025_10_31_ZIcI1_5mW_calib_before_label3d_dannce.mat\n",
      "mir_generate_param ran successfully.\n",
      "Updated Parquet file at /data/big_rim/rsync_dcc_sum/25Nov/2025_10_31/ZIcI1_5mW/folder_log.parquet with new status.\n",
      "\n",
      "‚úì Mirror parameter generation complete!\n"
     ]
    }
   ],
   "source": [
    "from utlis.exe_engine_utlis.comb_all_exe import sequential_process_and_update_mirgenparam\n",
    "\n",
    "# Specify which calibration to use, default is \"calib_before\"\n",
    "calib_folder_name = \"calib_before\"\n",
    "# calib_folder_name = \"calib_before_newintrinsics\"\n",
    "\n",
    "print(f\"Using calibration: {calib_folder_name}\\n\")\n",
    "sequential_process_and_update_mirgenparam(filtered_table, base_folder, calib_folder_name)\n",
    "\n",
    "print(\"\\n‚úì Mir parameter generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Camera Synchronization\n",
    "\n",
    "### When to run:\n",
    "Filter: `sync=0`\n",
    "\n",
    "### What this does:\n",
    "- Detects LED brightness transitions in video streams\n",
    "- Creates synchronized frame indices for multi-camera analysis\n",
    "- Generates validation plots showing brightness profiles\n",
    "\n",
    "\n",
    "### Synchronization protocol:\n",
    "The recording protocol includes **3 brightness drops** (LED switches)\n",
    "\n",
    "\n",
    "### Critical: Manual supervision required!\n",
    "\n",
    "**Why supervision matters:**\n",
    "- Initial frames may have fluctuating brightness, can cause false synchronization\n",
    "- Must visually verify sync looks correct\n",
    "\n",
    "**What to check in output plots:**\n",
    "- All cameras show clear brightness drops\n",
    "- First drop is clean (not during fluctuation period)\n",
    "\n",
    "### Expected output:\n",
    "\n",
    "**Generated files:**\n",
    "- `videos/6cam_sync.png` - Brightness profile plot (CHECK THIS, would also display)\n",
    "- `df*label3d_dannce.mat` - Synchronized frame indices updated to calibration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis.exe_engine_utlis.comb_all_exe import sequential_process_and_update_sync\n",
    "\n",
    "print(\"Starting camera synchronization...\\n\")\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: Check the generated sync plots for each session!\\n\")\n",
    "\n",
    "sequential_process_and_update_sync(filtered_table, base_folder, max_frames=800)\n",
    "\n",
    "print(\"\\n‚úì Synchronization complete!\")\n",
    "print(\"\\nüìä For later validations: Review 6cam_sync.png plots in each session's videos/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. COM Prediction & Validation\n",
    "\n",
    "COM (Center of Mass) prediction provides coarse animal localization needed for full pose estimation.\n",
    "Note, the validation script is adapted from https://github.com/Sooophy/dannce/tree/stroke_analysis/trace_protocol. For better validations and improvements, can adapt scripts from Anshuman Sabath.\n",
    "\n",
    "### Pipeline split:\n",
    "- **Single animal**: Standard COM prediction\n",
    "- **Social**: Modified pipeline for multi-animal tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6A. Single Animal COM Prediction\n",
    "\n",
    "### When to run:\n",
    "Filter: `social=0, com=0`\n",
    "\n",
    "### What this does:\n",
    "- Predicts 3D center-of-mass position for each frame\n",
    "- Uses sDANNCE network with `--predict_com` flag\n",
    "\n",
    "\n",
    "### Expected output:\n",
    "```\n",
    "Submitting job for /path/to/session1...\n",
    "  Job ID: 12345678\n",
    "Submitting job for /path/to/session2...\n",
    "  Job ID: 12345679\n",
    "...\n",
    "‚úì All jobs submitted\n",
    "```\n",
    "\n",
    "**Generated files:**\n",
    "- `COM/predict00/com3d.mat` - 3D COM predictions\n",
    "- Slurm output logs in session directory\n",
    "- Monitor jobs: `squeue -u $USER`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis.exe_engine_utlis.comb_all_exe import dispatch_slurm_jobs\n",
    "\n",
    "print(\"Submitting COM prediction jobs to HPC cluster...\\n\")\n",
    "\n",
    "dispatch_slurm_jobs(\n",
    "    base_path=base_folder,\n",
    "    table=filtered_table,\n",
    "    slurm_launch_file=\"/hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict.py\",\n",
    "    predict_flag=\"--predict_com\",\n",
    "    conda_env=\"sdannce\",\n",
    "    partition=\"scavenger-gpu\",\n",
    "    dry_run=False,\n",
    "    max_workers=6,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Jobs submitted!\")\n",
    "print(\"\\nüìä Monitor progress: squeue -u $USER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6B. Single Animal COM Validation\n",
    "\n",
    "### When to run:\n",
    "Filter: `social=0, com=1, com_vis=0`\n",
    "\n",
    "### What this does:\n",
    "- Generates trajectory plots and validation visualizations\n",
    "- Plots COM positions across frames\n",
    "- Identifies potential tracking errors (jumps, occlusions) (optional)\n",
    "- Creates frame-by-frame overlay videos (optional)\n",
    "- Updates status flag to `com_vis=1` upon completion\n",
    "\n",
    "### What to check:\n",
    "- **Trajectory smoothness**: Should show natural movement patterns, not squres\n",
    "- **No large jumps**: Sudden position changes indicate tracking errors\n",
    "\n",
    "### If COM looks bad:\n",
    "1. Add session path to exclusion list (e.g., `bad_com.txt`)\n",
    "2. This prevents bad COM from propagating to DANNCE prediction\n",
    "3. Optionally retrain/finetune COM network if many failures (the function's additional functions: `com_folder_name='COM/predict00', perform_jump_indices=True, perform_video_generation=False, perform_generate_com_video=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis.vis_valid_utlis.com_trag_updated import plot_com_all\n",
    "\n",
    "for_com_vis = filtered_table\n",
    "\n",
    "records = [\n",
    "    {\n",
    "        'date_folder': date_folder.as_py(),\n",
    "        'rec_file': rec_file.as_py()\n",
    "    }\n",
    "    for date_folder, rec_file in zip(for_com_vis['date_folder'], for_com_vis['rec_file'])\n",
    "]\n",
    "\n",
    "print(f\"Validating {len(records)} COM predictions...\\n\")\n",
    "\n",
    "for i, record in enumerate(records, 1):\n",
    "    base_path = f\"{base_folder}/{record['date_folder']}/{record['rec_file']}\"\n",
    "    print(f\"[{i}/{len(records)}] {base_path}\")\n",
    "    \n",
    "    try:\n",
    "        plot_com_all(base_path)\n",
    "        print(\"  ‚úì Complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úì COM validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6C. Social COM Prediction [NOTE: the social pipeline may be updated later based on Tianqing Li's pipeline]\n",
    "\n",
    "### When to run:\n",
    "Filter: `social=1, com=0`\n",
    "\n",
    "### What this does:\n",
    "- Predicts COM for multiple animals in social interaction sessions\n",
    "- Same computational setup as single animal\n",
    "\n",
    "### Differences from single animal:\n",
    "- Uses `slurm_launch_predict_social.py` instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis.exe_engine_utlis.comb_all_exe import dispatch_slurm_jobs\n",
    "\n",
    "print(\"Submitting social COM prediction jobs...\\n\")\n",
    "\n",
    "dispatch_slurm_jobs(\n",
    "    base_path=base_folder,\n",
    "    table=filtered_table,\n",
    "    slurm_launch_file=\"/hpc/group/tdunn/lq53/tianqing_pytorch_dannce/dannce_/slurm_launch_predict_social.py\",\n",
    "    predict_flag=\"--predict_com\",\n",
    "    conda_env=\"sdannce\",\n",
    "    partition=\"scavenger-gpu\",\n",
    "    dry_run=False,\n",
    "    max_workers=6,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Social COM jobs submitted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6D. Social COM Validation\n",
    "\n",
    "### When to run:\n",
    "Filter: `social=1, com=1, com_vis=0`\n",
    "\n",
    "### What this does:\n",
    "- Validates multi-animal COM tracking\n",
    "- Generates trajectory plots for each animal\n",
    "- Creates interaction distance plots\n",
    "- Optionally generates overlay videos\n",
    "\n",
    "### What to check:\n",
    "- **Identity maintenance**: Each animal's trajectory should be consistent\n",
    "- **No identity switches**: Animals shouldn't swap IDs mid-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis.vis_valid_utlis.scom_traga_utlis import plot_com_all_social\n",
    "\n",
    "for_com_vis = filtered_table\n",
    "\n",
    "records = [\n",
    "    {\n",
    "        'date_folder': date_folder.as_py(),\n",
    "        'rec_file': rec_file.as_py()\n",
    "    }\n",
    "    for date_folder, rec_file in zip(for_com_vis['date_folder'], for_com_vis['rec_file'])\n",
    "]\n",
    "\n",
    "print(f\"Validating {len(records)} social COM predictions...\\n\")\n",
    "\n",
    "for i, record in enumerate(records, 1):\n",
    "    base_path = f\"{base_folder}/{record['date_folder']}/{record['rec_file']}\"\n",
    "    print(f\"[{i}/{len(records)}] {base_path}\")\n",
    "    \n",
    "    try:\n",
    "        plot_com_all_social(base_path, perform_generate_com_video=True)\n",
    "        print(\"  ‚úì Complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úì Social COM validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. sDANNCE Prediction & Validation\n",
    "\n",
    "sDANNCE performs full 3D skeletal pose estimation.\n",
    "\n",
    "### Prerequisites:\n",
    "- ‚úì COM prediction completed and validated\n",
    "- ‚úì Bad COM sessions added to exclusion list (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7A. Single Animal DANNCE Prediction\n",
    "\n",
    "### When to run:\n",
    "Filter: `social=0, dannce=0`\n",
    "\n",
    "### What this does:\n",
    "- Predicts 3D coordinates for all anatomical keypoints\n",
    "- Optionally skips sessions with bad COM (from exclusion file)\n",
    "\n",
    "\n",
    "### Expected output:\n",
    "```\n",
    "Executing command: conda run -n sdannce python .../slurm_launch_predict.py ...\n",
    "Executing command: conda run -n sdannce python .../slurm_launch_predict.py ...\n",
    "Skipping: /path/to/bad_session is in the skip list\n",
    "...\n",
    "```\n",
    "\n",
    "**Generated files:**\n",
    "- `DANNCE/predict00/save_data_AVG0.mat` - 3D pose predictions\n",
    "\n",
    "\n",
    "\n",
    "### Exclusion list (optional):\n",
    "Create a text file with paths to skip (one per line):\n",
    "```\n",
    "2025_10_03/0single5_group2\n",
    "2025_10_10/0single4_group3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "\n",
    "for_dannce = filtered_table\n",
    "slurm_launch_file = \"/hpc/group/tdunn/lq53/251017_new_dannce_files/slurm_launch_predict.py\"\n",
    "\n",
    "def check_expdir(expdir):\n",
    "    \"\"\"Check if experiment directory exists.\"\"\"\n",
    "    if not os.path.exists(expdir):\n",
    "        print(f\"Skipping: Experiment directory {expdir} does not exist\")\n",
    "        return None\n",
    "    return expdir\n",
    "\n",
    "def run_command(base_path, date_folder, rec_file, partition='scavenger-gpu', dry_run=True):\n",
    "    \"\"\"Submit DANNCE prediction job via Slurm.\"\"\"\n",
    "    expdir_path = os.path.join(base_path, date_folder, rec_file)\n",
    "    \n",
    "    if check_expdir(expdir_path) is None:\n",
    "        return\n",
    "    \n",
    "    command = f\"conda run -n sdannce python {slurm_launch_file} --expdir {expdir_path} --predict_dannce --partition {partition}\"\n",
    "    \n",
    "    if dry_run:\n",
    "        print(f\"[DRY-RUN] Command: {command}\")\n",
    "    else:\n",
    "        print(f\"Executing command: {command}\")\n",
    "        os.system(command)\n",
    "\n",
    "# Optional: Load exclusion list\n",
    "txt_file = \"/hpc/group/tdunn/Bryan_Rigs/BigOpenField/lumi_novel_object_recog/bad_com.txt\"\n",
    "rel_paths_to_skip = set()\n",
    "\n",
    "if os.path.exists(txt_file):\n",
    "    print(f\"Loading exclusion list from: {txt_file}\\n\")\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            rel_path = line.strip()\n",
    "            if rel_path:\n",
    "                rel_paths_to_skip.add(rel_path)\n",
    "    print(f\"Will skip {len(rel_paths_to_skip)} sessions\\n\")\n",
    "\n",
    "# Prepare records\n",
    "records = [\n",
    "    {\n",
    "        'date_folder': date_folder.as_py(),\n",
    "        'rec_file': rec_file.as_py()\n",
    "    }\n",
    "    for date_folder, rec_file in zip(for_dannce['date_folder'], for_dannce['rec_file'])\n",
    "]\n",
    "\n",
    "max_concurrent_jobs = 4  # Parallel jobs\n",
    "dry_run = False  # Set to True for testing\n",
    "\n",
    "print(f\"Submitting {len(records)} DANNCE prediction jobs...\\n\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_concurrent_jobs) as executor:\n",
    "    futures = []\n",
    "    \n",
    "    for record in records:\n",
    "        rel_path = os.path.join(record['date_folder'], record['rec_file'])\n",
    "        expdir_path = os.path.join(base_folder, rel_path)\n",
    "        \n",
    "        if expdir_path in rel_paths_to_skip:\n",
    "            print(f\"Skipping: {rel_path} is in the skip list\")\n",
    "            continue\n",
    "        \n",
    "        futures.append(\n",
    "            executor.submit(run_command, base_folder, record['date_folder'], record['rec_file'], 'scavenger-gpu', dry_run)\n",
    "        )\n",
    "\n",
    "print(\"\\n‚úì DANNCE jobs submitted!\")\n",
    "print(\"\\nüìä Monitor: squeue -u $USER\")\n",
    "print(\"‚è±Ô∏è  Expected: ~30-60 min per session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7B. Single Animal DANNCE Validation\n",
    "\n",
    "### When to run:\n",
    "Filter: `social=0, dannce=1, dannce_vis=0`\n",
    "\n",
    "### What this does:\n",
    "- Generates 3D pose validation visualizations\n",
    "- Checks anatomical constraints (bone lengths, angles)\n",
    "- Creates skeleton overlay videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from useful_files.sophie_check_dannce_mir_modif import dannce_valid\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "for_dannce_vis = filtered_table\n",
    "\n",
    "records = [\n",
    "    {\n",
    "        'date_folder': date_folder.as_py(),\n",
    "        'rec_file': rec_file.as_py()\n",
    "    }\n",
    "    for date_folder, rec_file in zip(for_dannce_vis['date_folder'], for_dannce_vis['rec_file'])\n",
    "]\n",
    "\n",
    "print(f\"Validating {len(records)} DANNCE predictions...\\n\")\n",
    "\n",
    "def process_record(record):\n",
    "    \"\"\"Process a single DANNCE validation.\"\"\"\n",
    "    base_path = f\"{base_folder}/{record['date_folder']}/{record['rec_file']}\"\n",
    "    print(base_path)\n",
    "    try:\n",
    "        dannce_valid(base_path)\n",
    "        return f\"‚úì {base_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚úó {base_path}: {e}\"\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_record, record) for record in records]\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        print(result)\n",
    "\n",
    "print(\"\\n‚úì DANNCE validation complete!\")\n",
    "print(\"\\nüìä Review validation plots in DANNCE/predict00/vis/ folders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Social DANNCE (Coming Soon)\n",
    "\n",
    "### Current status:\n",
    "Social DANNCE prediction and validation workflows will be updated soon. \n",
    "\n",
    "### Available visualization:\n",
    "For social sessions with 3D poses, you can use:\n",
    "\n",
    "```python\n",
    "from utlis.vis_valid_utlis.social_dannce_vis import visualize_frames\n",
    "\n",
    "visualize_frames(incident_all_six_cam, config=C, out_name=\"all_incidents\")\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "- Config object (`C`) with session parameters\n",
    "- Incident frames identified from behavioral analysis\n",
    "- Multi-camera video frames extracted\n",
    "\n",
    "### Coming updates:\n",
    "- Streamlined social DANNCE prediction pipeline\n",
    "- Automated identity assignment validation\n",
    "- Social interaction metrics (distances, orientations)\n",
    "- Multi-animal skeleton overlays\n",
    "\n",
    "**Timeline:** Will be updated once workflow is finalized and tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Reference: Common Filter Combinations\n",
    "\n",
    "Copy-paste these filter blocks as needed, but note that if you start from raw there is no need to get more filter beuacse you have to rescan and reload to get updated status:\n",
    "\n",
    "### Raw Data Processing\n",
    "```python\n",
    "# Step 1: Generate mirror parameters\n",
    "conditions = [pc.equal(table['mir_generate_param'], '0')]\n",
    "\n",
    "# Step 2: Sync cameras\n",
    "conditions = [pc.equal(table['sync'], '0')]\n",
    "```\n",
    "\n",
    "### Single Animal Pipeline\n",
    "```python\n",
    "# COM prediction\n",
    "conditions = [\n",
    "    pc.equal(table['sync'], '1'),\n",
    "    pc.equal(table['social'], '0'),\n",
    "    pc.equal(table['com'], '0'),\n",
    "]\n",
    "\n",
    "# COM validation\n",
    "conditions = [\n",
    "    pc.equal(table['social'], '0'),\n",
    "    pc.equal(table['com'], '1'),\n",
    "    pc.equal(table['com_vis'], '0'),\n",
    "]\n",
    "\n",
    "# DANNCE prediction\n",
    "conditions = [\n",
    "    pc.equal(table['social'], '0'),\n",
    "    pc.equal(table['dannce'], '0'),\n",
    "    pc.equal(table['com'], '1'),\n",
    "    pc.equal(table['com_vis'], '1'),\n",
    "]\n",
    "\n",
    "# DANNCE validation\n",
    "conditions = [\n",
    "    pc.equal(table['social'], '0'),\n",
    "    pc.equal(table['dannce'], '1'),\n",
    "    pc.equal(table['dannce_vis'], '0'),\n",
    "    pc.equal(table['com'], '1'),\n",
    "    pc.equal(table['com_vis'], '1'),\n",
    "]\n",
    "```\n",
    "\n",
    "### Social Animal Pipeline\n",
    "```python\n",
    "# COM prediction\n",
    "conditions = [\n",
    "    pc.equal(table['sync'], '1'),\n",
    "    pc.equal(table['social'], '1'),\n",
    "    pc.equal(table['com'], '0'),\n",
    "]\n",
    "\n",
    "# COM validation\n",
    "conditions = [\n",
    "    pc.equal(table['social'], '1'),\n",
    "    pc.equal(table['com'], '1'),\n",
    "    pc.equal(table['com_vis'], '0'),\n",
    "]\n",
    "```\n",
    "\n",
    "### Utility Filters\n",
    "```python\n",
    "# Specific date\n",
    "conditions = [pc.equal(table['date_folder'], '2025_11_03')]\n",
    "\n",
    "# Date range (requires string matching)\n",
    "conditions = [pc.match_substring(table['date_folder'], '2025_10')]\n",
    "\n",
    "# Multiple conditions\n",
    "conditions = [\n",
    "    pc.equal(table['social'], '1'),\n",
    "    pc.equal(table['com'], '1'),\n",
    "    pc.match_substring(table['date_folder'], '2025_10'),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Troubleshooting Guide\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**PyArrow filtering errors:**\n",
    "- Always use strings for status values: `'0'`, `'1'` not `0`, `1`\n",
    "- Check column names match exactly (case-sensitive)\n",
    "- Use `all_df.to_pandas().columns.tolist()` to verify columns\n",
    "\n",
    "**Poor prediction quality:**\n",
    "- Review calibration - try different calib folder\n",
    "- Check synchronization plots for issues\n",
    "- Consider retraining networks on your data\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "- Review generated plots/videos for clues\n",
    "- Compare with successfully processed sessions\n",
    "- Check Slurm output files for error messages\n",
    "- Post an issue on github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Pipeline Summary\n",
    "\n",
    "| Step | Filter | Output Files |\n",
    "|------|--------|-------------|\n",
    "| **Scan** | N/A  `folder_log.parquet` |\n",
    "| **Mirror Gen** | `mir_generate_param=0` | calib file |\n",
    "| **Sync** | `sync=0` |  `6cam_sync.png`, udpated calib file |\n",
    "| **COM Pred** | `com=0` |  `com3d.mat` |\n",
    "| **COM Val** | `com=1, com_vis=0` | Trajectory plots |\n",
    "| **DANNCE Pred** | `dannce=0` |  `save_data_AVG0.mat` |\n",
    "| **DANNCE Val** | `dannce=1, dannce_vis=0` |  Validation plots |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Next Steps\n",
    "\n",
    "After completing this preprocessing pipeline:\n",
    "\n",
    "1. **Data Quality Check**: Review all validation plots systematically\n",
    "2. **Behavioral Analysis**: Extract kinematic features from 3D poses (you can try neuroposelib)\n",
    "3. **Statistical Analysis**: Analyze behavioral metrics across conditions\n",
    "4. **Neural Alignment**: Correlate behavior with calcium imaging data (see other tutorials with general loader)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Check function docstrings or review generated outputs for debugging clues.\n",
    "\n",
    "**Happy pre-processing! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbop241209_triptych",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
